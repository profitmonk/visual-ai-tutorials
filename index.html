<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Interactive Transformer Architecture Tutorials</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background: linear-gradient(135deg, #1a1a1a 0%, #2d2d2d 100%);
            color: #e0e0e0;
            line-height: 1.6;
        }
        
        .container {
            background: #ffffff;
            color: #2d2d2d;
            border-radius: 20px;
            padding: 30px;
            margin: 20px 0;
            border: 1px solid #e0e0e0;
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
        }
        
        .tutorial-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(320px, 1fr));
            gap: 30px;
            margin: 30px 0;
            align-items: stretch;
        }
        
        .tutorial-card {
            background: #f8f9fa;
            border: 2px solid #e9ecef;
            border-radius: 15px;
            padding: 25px;
            transition: all 0.3s ease;
            cursor: pointer;
            display: flex;
            flex-direction: column;
            height: 300px;
        }
        
        .tutorial-card:hover {
            border-color: #2d2d2d;
            transform: translateY(-5px);
            box-shadow: 0 8px 25px rgba(0, 0, 0, 0.15);
        }
        
        .tutorial-title {
            font-size: 1.3em;
            font-weight: bold;
            margin-bottom: 15px;
            color: #2d2d2d;
        }
        
        .tutorial-description {
            color: #555;
            margin-bottom: 15px;
            font-size: 14px;
            flex-grow: 1;
        }
        
        .tutorial-concepts {
            background: #2d2d2d;
            color: #e0e0e0;
            padding: 10px;
            border-radius: 8px;
            font-size: 12px;
            font-family: 'Courier New', monospace;
            margin-top: auto;
        }
        
        .tutorial-link {
            text-decoration: none;
            color: inherit;
        }
        
        .hero {
            text-align: center;
            padding: 40px 0;
        }
        
        .hero h1 {
            font-size: 2.5em;
            margin-bottom: 20px;
            background: linear-gradient(135deg, #2d2d2d, #1a1a1a);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        
        .hero p {
            font-size: 1.2em;
            color: #666;
            max-width: 800px;
            margin: 0 auto;
        }
        
        .features {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 40px 0;
        }
        
        .feature {
            text-align: center;
            padding: 20px;
        }
        
        .feature-icon {
            font-size: 2em;
            margin-bottom: 10px;
        }
        
        .feature-title {
            font-weight: bold;
            margin-bottom: 10px;
        }
        
        .github-link {
            background: #2d2d2d;
            color: white;
            padding: 15px 30px;
            border-radius: 10px;
            text-decoration: none;
            display: inline-block;
            margin: 20px 10px;
            font-weight: bold;
            transition: all 0.3s ease;
        }
        
        .github-link:hover {
            background: #1a1a1a;
            transform: translateY(-2px);
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.3);
        }
        
        .new-badge {
            background: #28a745;
            color: white;
            padding: 4px 8px;
            border-radius: 12px;
            font-size: 10px;
            font-weight: bold;
            text-transform: uppercase;
            margin-left: 10px;
        }
        
        .foundation-badge {
            background: #dc3545;
            color: white;
            padding: 4px 8px;
            border-radius: 12px;
            font-size: 10px;
            font-weight: bold;
            text-transform: uppercase;
            margin-left: 10px;
        }
        
        .series-badge {
            background: #6f42c1;
            color: white;
            padding: 4px 8px;
            border-radius: 12px;
            font-size: 10px;
            font-weight: bold;
            text-transform: uppercase;
            margin-left: 10px;
        }
        
        .section-header {
            text-align: center;
            margin: 50px 0 30px 0;
            padding: 20px;
            background: linear-gradient(135deg, #f8f9fa, #e9ecef);
            border-radius: 15px;
            border: 2px solid #dee2e6;
        }
        
        .section-title {
            font-size: 1.8em;
            font-weight: bold;
            color: #2d2d2d;
            margin-bottom: 10px;
        }
        
        .section-description {
            color: #666;
            font-size: 1.1em;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="hero">
            <h1>üß† Interactive Transformer Architecture Tutorials</h1>
            <p>Learn transformer architecture concepts through hands-on visualizations and step-by-step mathematical analysis</p>
            
            <a href="https://github.com/profitmonk/visual-ai-tutorials" class="github-link">
                üìÇ View on GitHub
            </a>
            <a href="https://github.com/profitmonk/visual-ai-tutorials/stargazers" class="github-link">
                ‚≠ê Star Repository
            </a>
        </div>
    </div>
    
    <div class="container">
        <div class="section-header">
            <div class="section-title">üèõÔ∏è Foundation Tutorials</div>
            <div class="section-description">Essential concepts and architectural understanding</div>
        </div>
        
        <div class="tutorial-grid">
            <a href="transformer-basics.html" class="tutorial-link">
                <div class="tutorial-card">
                    <div class="tutorial-title">üèóÔ∏è Transformer Basics: The Foundation <span class="foundation-badge">Start Here</span></div>
                    <div class="tutorial-description">
                        Essential foundation for understanding modern AI - from the revolutionary breakthrough to why transformers work so well. Covers the core architecture, three paradigms (BERT/GPT/T5), and interactive comparisons with older architectures.
                    </div>
                    <div class="tutorial-concepts">
                        Attention mechanism ‚Ä¢ Parallel processing ‚Ä¢ Architectural paradigms ‚Ä¢ AI evolution
                    </div>
                </div>
            </a>
            
            <a href="architecture-comparison.html" class="tutorial-link">
                <div class="tutorial-card">
                    <div class="tutorial-title">üìä Architecture Comparison: Modern LLM Designs <span class="new-badge">New</span></div>
                    <div class="tutorial-description">
                        Comprehensive comparison of modern LLM architectures across the industry. Real model analysis of GPT-4, Claude, Gemini, LLaMA, and more with design decisions breakdown and performance trade-offs.
                    </div>
                    <div class="tutorial-concepts">
                        Model comparison ‚Ä¢ Design trade-offs ‚Ä¢ Production considerations ‚Ä¢ Architectural evolution
                    </div>
                </div>
            </a>
            
            <a href="qkv-matrices.html" class="tutorial-link">
                <div class="tutorial-card">
                    <div class="tutorial-title">üéØ Q, K, V Matrix Dimensions</div>
                    <div class="tutorial-description">
                        Interactive exploration of attention mechanism matrix sizes and their relationship to model architecture with real model comparisons, matrix size calculations, and architecture analysis.
                    </div>
                    <div class="tutorial-concepts">
                        Attention matrices ‚Ä¢ Model dimensions ‚Ä¢ Memory scaling ‚Ä¢ Architecture comparison
                    </div>
                </div>
            </a>
            
            <a href="rope-tutorial.html" class="tutorial-link">
                <div class="tutorial-card">
                    <div class="tutorial-title">üåÄ RoPE: Rotary Position Embedding</div>
                    <div class="tutorial-description">
                        Comprehensive guide to understanding how transformers encode position information through rotation with visual dimension pairing, complete mathematical walkthrough, and interactive examples.
                    </div>
                    <div class="tutorial-concepts">
                        Position encoding ‚Ä¢ Dimension pairs ‚Ä¢ Rotation mathematics ‚Ä¢ Context scaling
                    </div>
                </div>
            </a>
        </div>
    </div>
    
    <div class="container">
        <div class="section-header">
            <div class="section-title">üîß Fine-tuning Mastery Series</div>
            <div class="section-description">Complete guide to efficient model adaptation and customization</div>
        </div>
        
        <div class="tutorial-grid">
            <a href="lora-tutorial.html" class="tutorial-link">
                <div class="tutorial-card">
                    <div class="tutorial-title">üîç LoRA: Low-Rank Adaptation Mathematics <span class="series-badge">Series 1</span></div>
                    <div class="tutorial-description">
                        Complete mathematical foundation of LoRA - the breakthrough technique for efficient fine-tuning. Interactive parameter calculator, matrix decomposition visualizer, and production deployment strategies.
                    </div>
                    <div class="tutorial-concepts">
                        Low-rank decomposition ‚Ä¢ Parameter efficiency ‚Ä¢ Rank selection ‚Ä¢ Adapter strategies
                    </div>
                </div>
            </a>
            
            <a href="finetuning-comparison.html" class="tutorial-link">
                <div class="tutorial-card">
                    <div class="tutorial-title">üéõÔ∏è Full Fine-tuning vs LoRA: Complete Comparison <span class="series-badge">Series 2</span></div>
                    <div class="tutorial-description">
                        Master the complete spectrum of fine-tuning approaches. Interactive layer freezing, catastrophic forgetting analysis, memory calculators, and smart decision framework for optimal approach selection.
                    </div>
                    <div class="tutorial-concepts">
                        Full fine-tuning ‚Ä¢ Layer freezing ‚Ä¢ Catastrophic forgetting ‚Ä¢ Resource optimization
                    </div>
                </div>
            </a>
            
            <a href="advanced-peft.html" class="tutorial-link">
                <div class="tutorial-card">
                    <div class="tutorial-title">üöÄ Advanced PEFT: QLoRA, DoRA & Modern Techniques <span class="series-badge">Series 3</span></div>
                    <div class="tutorial-description">
                        Cutting-edge Parameter-Efficient Fine-Tuning techniques. QLoRA's 4-bit quantization, DoRA weight decomposition, AdaLoRA adaptive allocation, and latest research developments.
                    </div>
                    <div class="tutorial-concepts">
                        Quantization mathematics ‚Ä¢ Advanced PEFT ‚Ä¢ Deployment optimization ‚Ä¢ Latest research
                    </div>
                </div>
            </a>
        </div>
    </div>
    
    <div class="container">
        <div class="section-header">
            <div class="section-title">‚ö° Core Mechanisms</div>
            <div class="section-description">Deep dives into transformer internals and processing</div>
        </div>
        
        <div class="tutorial-grid">
            <a href="complete-attention-mechanism.html" class="tutorial-link">
                <div class="tutorial-card">
                    <div class="tutorial-title">‚ö° Complete Attention Mechanism</div>
                    <div class="tutorial-description">
                        Interactive step-by-step walkthrough of how Q, K, V matrices work together in transformer attention, from matrix creation through final output with real examples.
                    </div>
                    <div class="tutorial-concepts">
                        Q√óK^T computation ‚Ä¢ Softmax normalization ‚Ä¢ Attention√óV ‚Ä¢ Matrix interactions
                    </div>
                </div>
            </a>
            
            <a href="attention-evolution.html" class="tutorial-link">
                <div class="tutorial-card">
                    <div class="tutorial-title">üîÑ Attention Mechanisms Evolution: MHA ‚Üí GQA ‚Üí MLA</div>
                    <div class="tutorial-description">
                        Complete evolution of attention mechanisms with KV caching foundation, memory optimization techniques, and deep dive into compression mathematics across all variants.
                    </div>
                    <div class="tutorial-concepts">
                        KV caching ‚Ä¢ Memory optimization ‚Ä¢ Grouped attention ‚Ä¢ Compression techniques ‚Ä¢ Evolution timeline
                    </div>
                </div>
            </a>
            
            <a href="text-generation-process.html" class="tutorial-link">
                <div class="tutorial-card">
                    <div class="tutorial-title">üöÄ Text Generation Process</div>
                    <div class="tutorial-description">
                        Complete mathematical walkthrough from attention output to next token prediction, including feed-forward networks, layer normalization, vocabulary projection, and sampling strategies.
                    </div>
                    <div class="tutorial-concepts">
                        FFN computation ‚Ä¢ Matrix flows ‚Ä¢ Vocabulary logits ‚Ä¢ Sampling strategies ‚Ä¢ Performance analysis
                    </div>
                </div>
            </a>
        </div>
    </div>
    
    <div class="container">
        <div class="section-header">
            <div class="section-title">üöÄ Advanced Topics</div>
            <div class="section-description">Scaling, optimization, and cutting-edge techniques</div>
        </div>
        
        <div class="tutorial-grid">
            <a href="mixture-of-experts.html" class="tutorial-link">
                <div class="tutorial-card">
                    <div class="tutorial-title">üéØ Mixture of Experts: Scaling Transformers Efficiently</div>
                    <div class="tutorial-description">
                        Interactive exploration of MoE scaling through sparsity, routing mechanics, expert specialization, load balancing, and real-world model analysis with cost-benefit considerations.
                    </div>
                    <div class="tutorial-concepts">
                        Sparse computation ‚Ä¢ Expert routing ‚Ä¢ Load balancing ‚Ä¢ Parameter scaling ‚Ä¢ Real MoE models
                    </div>
                </div>
            </a>
            
            <a href="context-length-impact.html" class="tutorial-link">
                <div class="tutorial-card">
                    <div class="tutorial-title">üìä Context Length Impact: Training vs Inference</div>
                    <div class="tutorial-description">
                        Mathematical analysis of why models trained on long contexts excel at shorter sequences with fixed vs dynamic components, RoPE frequency analysis, and performance metrics.
                    </div>
                    <div class="tutorial-concepts">
                        Context extension ‚Ä¢ Performance analysis ‚Ä¢ RoPE frequencies ‚Ä¢ Training vs inference
                    </div>
                </div>
            </a>
        </div>
    </div>
    
    <div class="container">
        <h2>‚ú® Tutorial Features</h2>
        
        <div class="features">
            <div class="feature">
                <div class="feature-icon">üì±</div>
                <div class="feature-title">Responsive Design</div>
                <div>Works on desktop, tablet, and mobile</div>
            </div>
            
            <div class="feature">
                <div class="feature-icon">üé®</div>
                <div class="feature-title">Interactive Visualizations</div>
                <div>Real-time calculations and demonstrations</div>
            </div>
            
            <div class="feature">
                <div class="feature-icon">üî¢</div>
                <div class="feature-title">Mathematical Precision</div>
                <div>Step-by-step formulas with actual numbers</div>
            </div>
            
            <div class="feature">
                <div class="feature-icon">üìä</div>
                <div class="feature-title">Real Model Data</div>
                <div>Architecture specs from production models</div>
            </div>
            
            <div class="feature">
                <div class="feature-icon">üéõÔ∏è</div>
                <div class="feature-title">Configurable Examples</div>
                <div>Adjust parameters to see immediate effects</div>
            </div>
            
            <div class="feature">
                <div class="feature-icon">üîß</div>
                <div class="feature-title">Production Ready</div>
                <div>Deployment strategies and resource planning</div>
            </div>
        </div>
    </div>
    
    <div class="container">
        <h2>üéØ Target Audience</h2>
        <ul>
            <li><strong>AI/ML Engineers</strong> learning transformer internals and fine-tuning strategies</li>
            <li><strong>Researchers</strong> studying attention mechanisms, PEFT techniques, and position encoding</li>
            <li><strong>Students</strong> in NLP/deep learning courses</li>
            <li><strong>Developers</strong> working with LLMs who want to understand underlying mathematics</li>
            <li><strong>Practitioners</strong> fine-tuning models for production deployment</li>
            <li><strong>Anyone curious</strong> about how modern AI models like GPT, Claude, and Gemini work</li>
        </ul>
    </div>
    
    <div class="container">
        <h2>üéì Recommended Learning Path</h2>
        
        <h3>üèõÔ∏è Foundation Phase</h3>
        <ol>
            <li><strong>üèóÔ∏è Transformer Basics</strong> - Understand the revolutionary breakthrough and foundation</li>
            <li><strong>üìä Architecture Comparison</strong> - Learn how modern LLMs differ and why</li>
            <li><strong>üéØ Q, K, V Matrix Dimensions</strong> - Understand the basic building blocks</li>
            <li><strong>üåÄ RoPE: Rotary Position Embedding</strong> - Learn how position is encoded</li>
        </ol>
        
        <h3>‚ö° Core Mechanisms Phase</h3>
        <ol start="5">
            <li><strong>‚ö° Complete Attention Mechanism</strong> - See how Q, K, V work together</li>
            <li><strong>üîÑ Attention Mechanisms Evolution</strong> - Learn memory optimization and scaling techniques</li>
            <li><strong>üöÄ Text Generation Process</strong> - Complete pipeline from attention to tokens</li>
        </ol>
        
        <h3>üîß Fine-tuning Mastery Phase</h3>
        <ol start="8">
            <li><strong>üîç LoRA Mathematics</strong> - Master the most popular PEFT technique</li>
            <li><strong>üéõÔ∏è Full Fine-tuning vs LoRA</strong> - Complete comparison and decision framework</li>
            <li><strong>üöÄ Advanced PEFT</strong> - Cutting-edge techniques (QLoRA, DoRA, etc.)</li>
        </ol>
        
        <h3>üöÄ Advanced Topics Phase</h3>
        <ol start="11">
            <li><strong>üéØ Mixture of Experts</strong> - Advanced scaling through sparse computation</li>
            <li><strong>üìä Context Length Impact</strong> - Advanced concepts about training vs inference</li>
        </ol>
    </div>
    
    <div class="container">
        <h2>üõ†Ô∏è Technology Stack</h2>
        <ul>
            <li><strong>Pure HTML/CSS/JavaScript</strong> - No frameworks, works anywhere</li>
            <li><strong>Interactive calculations</strong> - Real-time mathematical demonstrations</li>
            <li><strong>Responsive design</strong> - Mobile-friendly layouts</li>
            <li><strong>GitHub Pages ready</strong> - Deploy with zero configuration</li>
        </ul>
    </div>
    
    <div class="container" style="text-align: center;">
        <h2>‚≠ê Star this repository if these tutorials helped you understand transformers and fine-tuning better!</h2>
        
        <a href="https://github.com/profitmonk/visual-ai-tutorials" class="github-link">
            üöÄ Get Started Now
        </a>
    </div>
</body>
</html>
