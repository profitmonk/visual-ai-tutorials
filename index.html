<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Interactive Transformer Architecture Tutorials</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background: linear-gradient(135deg, #1a1a1a 0%, #2d2d2d 100%);
            color: #e0e0e0;
            line-height: 1.6;
        }
        
        .container {
            background: #ffffff;
            color: #2d2d2d;
            border-radius: 20px;
            padding: 30px;
            margin: 20px 0;
            border: 1px solid #e0e0e0;
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
        }
        
        .tutorial-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 30px;
            margin: 30px 0;
            align-items: stretch;
        }
        
        .tutorial-card {
            background: #f8f9fa;
            border: 2px solid #e9ecef;
            border-radius: 15px;
            padding: 25px;
            transition: all 0.3s ease;
            cursor: pointer;
            display: flex;
            flex-direction: column;
            height: 280px;
        }
        
        .tutorial-card:hover {
            border-color: #2d2d2d;
            transform: translateY(-5px);
            box-shadow: 0 8px 25px rgba(0, 0, 0, 0.15);
        }
        
        .tutorial-title {
            font-size: 1.3em;
            font-weight: bold;
            margin-bottom: 15px;
            color: #2d2d2d;
        }
        
        .tutorial-description {
            color: #555;
            margin-bottom: 15px;
            font-size: 14px;
            flex-grow: 1;
        }
        
        .tutorial-concepts {
            background: #2d2d2d;
            color: #e0e0e0;
            padding: 10px;
            border-radius: 8px;
            font-size: 12px;
            font-family: 'Courier New', monospace;
            margin-top: auto;
        }
        
        .tutorial-link {
            text-decoration: none;
            color: inherit;
        }
        
        .hero {
            text-align: center;
            padding: 40px 0;
        }
        
        .hero h1 {
            font-size: 2.5em;
            margin-bottom: 20px;
            background: linear-gradient(135deg, #2d2d2d, #1a1a1a);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        
        .hero p {
            font-size: 1.2em;
            color: #666;
            max-width: 800px;
            margin: 0 auto;
        }
        
        .features {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 40px 0;
        }
        
        .feature {
            text-align: center;
            padding: 20px;
        }
        
        .feature-icon {
            font-size: 2em;
            margin-bottom: 10px;
        }
        
        .feature-title {
            font-weight: bold;
            margin-bottom: 10px;
        }
        
        .github-link {
            background: #2d2d2d;
            color: white;
            padding: 15px 30px;
            border-radius: 10px;
            text-decoration: none;
            display: inline-block;
            margin: 20px 10px;
            font-weight: bold;
            transition: all 0.3s ease;
        }
        
        .github-link:hover {
            background: #1a1a1a;
            transform: translateY(-2px);
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.3);
        }
        
        .new-badge {
            background: #28a745;
            color: white;
            padding: 4px 8px;
            border-radius: 12px;
            font-size: 10px;
            font-weight: bold;
            text-transform: uppercase;
            margin-left: 10px;
        }
        
        .foundation-badge {
            background: #dc3545;
            color: white;
            padding: 4px 8px;
            border-radius: 12px;
            font-size: 10px;
            font-weight: bold;
            text-transform: uppercase;
            margin-left: 10px;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="hero">
            <h1>üß† Interactive Transformer Architecture Tutorials</h1>
            <p>Learn transformer architecture concepts through hands-on visualizations and step-by-step mathematical analysis</p>
            
            <a href="https://github.com/profitmonk/visual-ai-tutorials" class="github-link">
                üìÇ View on GitHub
            </a>
            <a href="https://github.com/profitmonk/visual-ai-tutorials/stargazers" class="github-link">
                ‚≠ê Star Repository
            </a>
        </div>
    </div>
    
    <div class="container">
        <h2>üöÄ Available Tutorials</h2>
        
        <div class="tutorial-grid">
            <a href="transformer-basics.html" class="tutorial-link">
                <div class="tutorial-card">
                    <div class="tutorial-title">üèóÔ∏è Transformer Basics: The Foundation <span class="foundation-badge">Start Here</span></div>
                    <div class="tutorial-description">
                        Essential foundation for understanding modern AI - from the revolutionary breakthrough to why transformers work so well. Covers the core architecture, three paradigms (BERT/GPT/T5), and interactive comparisons with older architectures.
                    </div>
                    <div class="tutorial-concepts">
                        Attention mechanism ‚Ä¢ Parallel processing ‚Ä¢ Architectural paradigms ‚Ä¢ AI evolution
                    </div>
                </div>
            </a>
            
            <a href="architecture-comparison.html" class="tutorial-link">
                <div class="tutorial-card">
                    <div class="tutorial-title">üìä Architecture Comparison: Modern LLM Designs <span class="new-badge">New</span></div>
                    <div class="tutorial-description">
                        Comprehensive comparison of modern LLM architectures across the industry. Real model analysis of GPT-4, Claude, Gemini, LLaMA, and more with design decisions breakdown and performance trade-offs.
                    </div>
                    <div class="tutorial-concepts">
                        Model comparison ‚Ä¢ Design trade-offs ‚Ä¢ Production considerations ‚Ä¢ Architectural evolution
                    </div>
                </div>
            </a>
            
            <a href="qkv-matrices.html" class="tutorial-link">
                <div class="tutorial-card">
                    <div class="tutorial-title">üéØ Q, K, V Matrix Dimensions</div>
                    <div class="tutorial-description">
                        Interactive exploration of attention mechanism matrix sizes and their relationship to model architecture with real model comparisons, matrix size calculations, and architecture analysis.
                    </div>
                    <div class="tutorial-concepts">
                        Attention matrices ‚Ä¢ Model dimensions ‚Ä¢ Memory scaling ‚Ä¢ Architecture comparison
                    </div>
                </div>
            </a>
            
            <a href="rope-tutorial.html" class="tutorial-link">
                <div class="tutorial-card">
                    <div class="tutorial-title">üåÄ RoPE: Rotary Position Embedding</div>
                    <div class="tutorial-description">
                        Comprehensive guide to understanding how transformers encode position information through rotation with visual dimension pairing, complete mathematical walkthrough, and interactive examples with up to 128D embeddings.
                    </div>
                    <div class="tutorial-concepts">
                        Position encoding ‚Ä¢ Dimension pairs ‚Ä¢ Rotation mathematics ‚Ä¢ Context scaling
                    </div>
                </div>
            </a>
            
            <a href="complete-attention-mechanism.html" class="tutorial-link">
                <div class="tutorial-card">
                    <div class="tutorial-title">‚ö° Complete Attention Mechanism</div>
                    <div class="tutorial-description">
                        Interactive step-by-step walkthrough of how Q, K, V matrices work together in transformer attention, from matrix creation through final output with real examples.
                    </div>
                    <div class="tutorial-concepts">
                        Q√óK^T computation ‚Ä¢ Softmax normalization ‚Ä¢ Attention√óV ‚Ä¢ Matrix interactions
                    </div>
                </div>
            </a>
            
            <a href="attention-evolution.html" class="tutorial-link">
                <div class="tutorial-card">
                    <div class="tutorial-title">üîÑ Attention Mechanisms Evolution: MHA ‚Üí GQA ‚Üí MLA</div>
                    <div class="tutorial-description">
                        Complete evolution of attention mechanisms from Multi-Head Attention through Grouped Query Attention to Multi-Head Latent Attention, with deep dive into KV caching, memory optimization, and compression techniques.
                    </div>
                    <div class="tutorial-concepts">
                        KV caching ‚Ä¢ Memory optimization ‚Ä¢ Grouped attention ‚Ä¢ Compression techniques ‚Ä¢ Evolution timeline
                    </div>
                </div>
            </a>
            
            <a href="text-generation-process.html" class="tutorial-link">
                <div class="tutorial-card">
                    <div class="tutorial-title">üöÄ Text Generation Process</div>
                    <div class="tutorial-description">
                        Complete mathematical walkthrough from attention output to next token prediction, including feed-forward networks, layer normalization, vocabulary projection, and sampling strategies with exact matrix computations.
                    </div>
                    <div class="tutorial-concepts">
                        FFN computation ‚Ä¢ Matrix flows ‚Ä¢ Vocabulary logits ‚Ä¢ Sampling strategies ‚Ä¢ Performance analysis
                    </div>
                </div>
            </a>
            
            <a href="mixture-of-experts.html" class="tutorial-link">
                <div class="tutorial-card">
                    <div class="tutorial-title">üéØ Mixture of Experts: Scaling Transformers Efficiently</div>
                    <div class="tutorial-description">
                        Interactive exploration of how MoE scales transformer models through sparsity, routing, and selective expert activation. Covers dense vs sparse computation, router mechanics, expert specialization, and real-world MoE architectures.
                    </div>
                    <div class="tutorial-concepts">
                        Sparse computation ‚Ä¢ Expert routing ‚Ä¢ Load balancing ‚Ä¢ Parameter scaling ‚Ä¢ Real MoE models
                    </div>
                </div>
            </a>
            
            <a href="context-length-impact.html" class="tutorial-link">
                <div class="tutorial-card">
                    <div class="tutorial-title">üìä Context Length Impact: Training vs Inference</div>
                    <div class="tutorial-description">
                        Mathematical analysis of why models trained on long contexts excel at shorter sequences with fixed vs dynamic components, RoPE frequency analysis, and performance metrics.
                    </div>
                    <div class="tutorial-concepts">
                        Context extension ‚Ä¢ Performance analysis ‚Ä¢ RoPE frequencies ‚Ä¢ Training vs inference
                    </div>
                </div>
            </a>
        </div>
    </div>
    
    <div class="container">
        <h2>‚ú® Tutorial Features</h2>
        
        <div class="features">
            <div class="feature">
                <div class="feature-icon">üì±</div>
                <div class="feature-title">Responsive Design</div>
                <div>Works on desktop, tablet, and mobile</div>
            </div>
            
            <div class="feature">
                <div class="feature-icon">üé®</div>
                <div class="feature-title">Interactive Visualizations</div>
                <div>Real-time calculations and demonstrations</div>
            </div>
            
            <div class="feature">
                <div class="feature-icon">üî¢</div>
                <div class="feature-title">Mathematical Precision</div>
                <div>Step-by-step formulas with actual numbers</div>
            </div>
            
            <div class="feature">
                <div class="feature-icon">üìä</div>
                <div class="feature-title">Real Model Data</div>
                <div>Architecture specs from production models</div>
            </div>
            
            <div class="feature">
                <div class="feature-icon">üéõÔ∏è</div>
                <div class="feature-title">Configurable Examples</div>
                <div>Adjust parameters to see immediate effects</div>
            </div>
            
            <div class="feature">
                <div class="feature-icon">üìö</div>
                <div class="feature-title">Educational Focus</div>
                <div>Designed for learning, not just reference</div>
            </div>
        </div>
    </div>
    
    <div class="container">
        <h2>üéØ Target Audience</h2>
        <ul>
            <li><strong>AI/ML Engineers</strong> learning transformer internals</li>
            <li><strong>Researchers</strong> studying attention mechanisms and position encoding</li>
            <li><strong>Students</strong> in NLP/deep learning courses</li>
            <li><strong>Developers</strong> working with LLMs who want to understand the underlying math</li>
            <li><strong>Anyone curious</strong> about how modern AI models like GPT, Claude, and Gemini work</li>
        </ul>
    </div>
    
    <div class="container">
        <h2>üéì Learning Path</h2>
        <p><strong>Recommended order for maximum understanding:</strong></p>
        <ol>
            <li><strong>üèóÔ∏è Transformer Basics</strong> - Understand the revolutionary breakthrough and foundation</li>
            <li><strong>üìä Architecture Comparison</strong> - Learn how modern LLMs differ and why</li>
            <li><strong>üéØ Q, K, V Matrix Dimensions</strong> - Understand the basic building blocks</li>
            <li><strong>üåÄ RoPE: Rotary Position Embedding</strong> - Learn how position is encoded</li>
            <li><strong>‚ö° Complete Attention Mechanism</strong> - See how Q, K, V work together</li>
            <li><strong>üîÑ Attention Mechanisms Evolution</strong> - Learn memory optimization and scaling techniques</li>
            <li><strong>üöÄ Text Generation Process</strong> - Complete pipeline from attention to tokens</li>
            <li><strong>üéØ Mixture of Experts</strong> - Advanced scaling through sparse computation</li>
            <li><strong>üìä Context Length Impact</strong> - Advanced concepts about training vs inference</li>
        </ol>
    </div>
    
    <div class="container">
        <h2>üõ†Ô∏è Technology Stack</h2>
        <ul>
            <li><strong>Pure HTML/CSS/JavaScript</strong> - No frameworks, works anywhere</li>
            <li><strong>Interactive calculations</strong> - Real-time mathematical demonstrations</li>
            <li><strong>Responsive design</strong> - Mobile-friendly layouts</li>
            <li><strong>GitHub Pages ready</strong> - Deploy with zero configuration</li>
        </ul>
    </div>
    
    <div class="container" style="text-align: center;">
        <h2>‚≠ê Star this repository if these tutorials helped you understand transformers better!</h2>
        
        <a href="https://github.com/profitmonk/visual-ai-tutorials" class="github-link">
            üöÄ Get Started Now
        </a>
    </div>
</body>
</html>
