<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Transformer Basics: The Foundation</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background: linear-gradient(135deg, #1a1a1a 0%, #2d2d2d 100%);
            color: #e0e0e0;
            line-height: 1.6;
        }
        
        .container {
            background: #ffffff;
            color: #2d2d2d;
            border-radius: 20px;
            padding: 30px;
            margin: 20px 0;
            border: 1px solid #e0e0e0;
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
        }
        
        .nav-bar {
            background: #2d2d2d;
            color: white;
            padding: 15px 30px;
            border-radius: 15px;
            margin: 20px 0;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }
        
        .nav-home {
            background: #ffffff;
            color: #2d2d2d;
            padding: 8px 16px;
            border-radius: 6px;
            text-decoration: none;
            font-weight: bold;
            transition: all 0.3s ease;
        }
        
        .nav-home:hover {
            background: #f8f9fa;
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.2);
        }
        
        .nav-title {
            font-size: 1.2em;
            font-weight: bold;
        }
        
        .step {
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            padding: 20px;
            margin: 15px 0;
            border-radius: 15px;
            border-left: 4px solid #2d2d2d;
        }
        
        .example-box {
            background: #2d2d2d;
            color: #e0e0e0;
            padding: 15px;
            border-radius: 10px;
            margin: 10px 0;
            font-family: 'Courier New', monospace;
            overflow-x: auto;
            border: 1px solid #4a4a4a;
        }
        
        .controls {
            display: flex;
            gap: 15px;
            margin: 20px 0;
            flex-wrap: wrap;
        }
        
        .control-group {
            display: flex;
            flex-direction: column;
            gap: 5px;
        }
        
        button {
            background: #2d2d2d;
            border: none;
            color: white;
            padding: 12px 24px;
            border-radius: 8px;
            cursor: pointer;
            font-weight: bold;
            transition: all 0.3s ease;
        }
        
        button:hover {
            background: #1a1a1a;
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.2);
        }
        
        input, select {
            padding: 8px 12px;
            border: 1px solid #dadce0;
            border-radius: 6px;
            background: #ffffff;
            color: #2d2d2d;
        }
        
        .warning {
            background: #fff3cd;
            border-left: 4px solid #ffc107;
            color: #856404;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
        }
        
        .success {
            background: #d4edda;
            border-left: 4px solid #28a745;
            color: #155724;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
        }
        
        .info {
            background: #d1ecf1;
            border-left: 4px solid #17a2b8;
            color: #0c5460;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
        }
        
        .danger {
            background: #f8d7da;
            border-left: 4px solid #dc3545;
            color: #721c24;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 15px 0;
            background: #ffffff;
            border: 1px solid #e9ecef;
            border-radius: 8px;
            overflow: hidden;
        }
        
        th {
            background: #2d2d2d;
            color: white;
            padding: 12px;
            text-align: center;
            font-weight: bold;
        }
        
        td {
            padding: 12px;
            text-align: center;
            border-bottom: 1px solid #e9ecef;
        }
        
        .comparison-winner {
            background: #d4edda;
            font-weight: bold;
        }
        
        .comparison-loser {
            background: #f8d7da;
        }
        
        .math-formula {
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            padding: 15px;
            border-radius: 8px;
            margin: 10px 0;
            font-family: 'Courier New', monospace;
            text-align: center;
            font-size: 14px;
        }
        
        .architecture-flow {
            display: flex;
            flex-direction: column;
            align-items: center;
            margin: 30px 0;
            padding: 20px;
            background: #f8f9fa;
            border-radius: 15px;
            border: 2px solid #e9ecef;
        }
        
        .flow-box {
            background: #2d2d2d;
            color: white;
            padding: 15px 25px;
            margin: 8px;
            border-radius: 10px;
            font-weight: bold;
            text-align: center;
            cursor: pointer;
            transition: all 0.3s ease;
            min-width: 180px;
        }
        
        .flow-box:hover {
            background: #1a1a1a;
            transform: scale(1.05);
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.3);
        }
        
        .flow-box.highlight {
            background: #28a745;
            animation: pulse 2s infinite;
        }
        
        @keyframes pulse {
            0% { opacity: 1; }
            50% { opacity: 0.7; }
            100% { opacity: 1; }
        }
        
        .flow-arrow {
            font-size: 24px;
            color: #2d2d2d;
            margin: 5px 0;
        }
        
        .attention-demo {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }
        
        .attention-card {
            background: #f8f9fa;
            border: 2px solid #e9ecef;
            border-radius: 12px;
            padding: 20px;
            text-align: center;
            transition: all 0.3s ease;
        }
        
        .attention-card:hover {
            border-color: #2d2d2d;
            transform: translateY(-5px);
            box-shadow: 0 8px 25px rgba(0, 0, 0, 0.15);
        }
        
        .attention-card.active {
            border-color: #28a745;
            background: #d4edda;
        }
        
        .attention-title {
            font-size: 1.1em;
            font-weight: bold;
            margin-bottom: 10px;
            color: #2d2d2d;
        }
        
        .attention-value {
            font-size: 1.5em;
            font-weight: bold;
            color: #dc3545;
            margin: 8px 0;
        }
        
        .model-timeline {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin: 30px 0;
            padding: 20px;
            background: #f8f9fa;
            border-radius: 12px;
            border: 2px solid #e9ecef;
        }
        
        .timeline-item {
            text-align: center;
            flex: 1;
            position: relative;
            cursor: pointer;
            transition: all 0.3s ease;
        }
        
        .timeline-item:hover {
            transform: scale(1.1);
        }
        
        .timeline-item:not(:last-child)::after {
            content: '→';
            position: absolute;
            right: -25px;
            top: 30%;
            font-size: 24px;
            color: #2d2d2d;
            font-weight: bold;
        }
        
        .timeline-year {
            background: #2d2d2d;
            color: white;
            padding: 6px 12px;
            border-radius: 15px;
            font-weight: bold;
            margin-bottom: 8px;
            display: inline-block;
            font-size: 12px;
        }
        
        .timeline-model {
            font-weight: bold;
            color: #2d2d2d;
            margin-bottom: 5px;
            font-size: 14px;
        }
        
        .timeline-desc {
            font-size: 11px;
            color: #666;
        }
        
        .paradigm-comparison {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }
        
        .paradigm-card {
            background: #f8f9fa;
            border: 2px solid #e9ecef;
            border-radius: 12px;
            padding: 20px;
            transition: all 0.3s ease;
        }
        
        .paradigm-card.selected {
            border-color: #28a745;
            background: #d4edda;
        }
        
        .paradigm-title {
            font-size: 1.2em;
            font-weight: bold;
            margin-bottom: 15px;
            color: #2d2d2d;
            text-align: center;
        }
        
        .paradigm-details {
            background: #2d2d2d;
            color: #e0e0e0;
            padding: 15px;
            border-radius: 8px;
            font-family: 'Courier New', monospace;
            font-size: 12px;
            margin-top: 10px;
        }
        
        .interactive-attention {
            background: #f8f9fa;
            border: 2px solid #e9ecef;
            border-radius: 12px;
            padding: 20px;
            margin: 20px 0;
        }
        
        .token-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(80px, 1fr));
            gap: 10px;
            margin: 15px 0;
        }
        
        .token-box {
            background: #2d2d2d;
            color: white;
            padding: 10px;
            border-radius: 8px;
            text-align: center;
            font-weight: bold;
            cursor: pointer;
            transition: all 0.3s ease;
        }
        
        .token-box:hover {
            background: #1a1a1a;
            transform: scale(1.1);
        }
        
        .token-box.selected {
            background: #28a745;
        }
        
        .attention-matrix {
            display: grid;
            grid-template-columns: repeat(6, 1fr);
            gap: 5px;
            margin: 20px 0;
            max-width: 400px;
            margin-left: auto;
            margin-right: auto;
        }
        
        .attention-cell {
            aspect-ratio: 1;
            background: #e9ecef;
            border-radius: 4px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 10px;
            font-weight: bold;
            transition: all 0.3s ease;
        }
        
        .breakthrough-comparison {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }
        
        .breakthrough-card {
            background: #f8f9fa;
            border: 2px solid #e9ecef;
            border-radius: 12px;
            padding: 20px;
            text-align: center;
        }
        
        .breakthrough-card.transformer {
            border-color: #28a745;
            background: #d4edda;
        }
        
        .breakthrough-title {
            font-size: 1.1em;
            font-weight: bold;
            margin-bottom: 15px;
            color: #2d2d2d;
        }
        
        .breakthrough-pros {
            color: #28a745;
            margin: 10px 0;
        }
        
        .breakthrough-cons {
            color: #dc3545;
            margin: 10px 0;
        }
    </style>
</head>
<body>
    <div class="nav-bar">
        <div class="nav-title">🏗️ Transformer Basics: The Foundation</div>
        <a href="index.html" class="nav-home">🏠 Home</a>
    </div>

    <div class="container">
        <h1>🏗️ Transformer Basics: The Foundation</h1>
        <p>Understand the revolutionary architecture that changed AI forever - from the core breakthrough to why it works so well</p>
    </div>

    <div class="container">
        <h2>🚨 The Problem: Why RNNs and CNNs Weren't Enough</h2>
        
        <div class="step">
            <h3>💔 The Sequential Processing Bottleneck</h3>
            
            <div class="example-box">
                <strong>❌ RNN Processing (The Old Way):</strong><br>
                Input: "The cat sat on the mat"<br><br>
                Step 1: Process "The" → hidden_state_1<br>
                Step 2: Process "cat" + hidden_state_1 → hidden_state_2<br>
                Step 3: Process "sat" + hidden_state_2 → hidden_state_3<br>
                Step 4: Process "on" + hidden_state_3 → hidden_state_4<br>
                Step 5: Process "the" + hidden_state_4 → hidden_state_5<br>
                Step 6: Process "mat" + hidden_state_5 → hidden_state_6<br><br>
                <strong>Problem:</strong> Must wait for each step! Can't parallelize!
            </div>
            
            <div class="example-box">
                <strong>❌ CNN Processing (Limited Context):</strong><br>
                Uses sliding windows (kernels) to process local patterns<br>
                Window size 3: ["The", "cat", "sat"] → pattern<br>
                Window size 3: ["cat", "sat", "on"] → pattern<br>
                Window size 3: ["sat", "on", "the"] → pattern<br><br>
                <strong>Problem:</strong> "The" and "mat" never directly interact!
            </div>
        </div>
        
        <div class="breakthrough-comparison">
            <div class="breakthrough-card">
                <div class="breakthrough-title">🐌 RNNs/LSTMs</div>
                <div class="breakthrough-pros">✅ Can handle any length</div>
                <div class="breakthrough-pros">✅ Good for sequences</div>
                <div class="breakthrough-cons">❌ Sequential processing (slow)</div>
                <div class="breakthrough-cons">❌ Vanishing gradients</div>
                <div class="breakthrough-cons">❌ Forgets long-term info</div>
            </div>
            
            <div class="breakthrough-card">
                <div class="breakthrough-title">🔍 CNNs</div>
                <div class="breakthrough-pros">✅ Parallel processing</div>
                <div class="breakthrough-pros">✅ Good for local patterns</div>
                <div class="breakthrough-cons">❌ Limited receptive field</div>
                <div class="breakthrough-cons">❌ Hard to capture long dependencies</div>
                <div class="breakthrough-cons">❌ Fixed window sizes</div>
            </div>
            
            <div class="breakthrough-card transformer">
                <div class="breakthrough-title">⚡ Transformers</div>
                <div class="breakthrough-pros">✅ Fully parallel processing</div>
                <div class="breakthrough-pros">✅ Direct long-range connections</div>
                <div class="breakthrough-pros">✅ No vanishing gradients</div>
                <div class="breakthrough-pros">✅ Scales beautifully</div>
                <div class="breakthrough-cons">❌ Quadratic memory (attention)</div>
            </div>
        </div>
        
        <div class="danger">
            <strong>🎯 The Core Problem:</strong> Before 2017, AI couldn't efficiently process sequences in parallel while maintaining long-range dependencies. This fundamentally limited how big and capable language models could become.
        </div>
    </div>

    <div class="container">
        <h2>💡 The Breakthrough: "Attention is All You Need"</h2>
        
        <div class="step">
            <h3>🧠 The Revolutionary Insight</h3>
            
            <div class="success">
                <strong>🎯 Core Breakthrough:</strong> What if we could train on entire sequences in parallel instead of processing them one token at a time?
            </div>
            
            <div class="warning">
                <strong>⚠️ Key Distinction:</strong> The "parallel processing" refers to TRAINING, not inference. During generation, transformers still produce tokens one at a time (autoregressive). The revolution is in how they LEARN!
            </div>
            
            <div class="example-box">
                <strong>✅ Transformer Training (The New Way):</strong><br>
                Training Input: "The cat sat on the mat"<br><br>
                ALL tokens processed simultaneously DURING TRAINING:<br>
                • Predict "cat" after "The" (position 1)<br>
                • Predict "sat" after "The cat" (position 2)<br>
                • Predict "on" after "The cat sat" (position 3)<br>
                • All predictions computed in parallel!<br><br>
                <strong>Result:</strong> Parallel training + perfect memory!
            </div>
            
            <div class="interactive-attention">
                <h4>🎮 Interactive Attention Demo</h4>
                <p>Click on any token to see what it attends to:</p>
                
                <div class="token-grid">
                    <div class="token-box" onclick="showAttention(0)">The</div>
                    <div class="token-box" onclick="showAttention(1)">cat</div>
                    <div class="token-box" onclick="showAttention(2)">sat</div>
                    <div class="token-box" onclick="showAttention(3)">on</div>
                    <div class="token-box" onclick="showAttention(4)">the</div>
                    <div class="token-box" onclick="showAttention(5)">mat</div>
                </div>
                
                <div id="attentionResult"></div>
            </div>
        </div>
        
        <div class="step">
            <h3>⚡ Why This Works So Well</h3>
            
            <div class="math-formula">
                <strong>Self-Attention (Simplified):</strong><br><br>
                For each token, compute:<br>
                • <strong>Query (Q):</strong> "What am I looking for?"<br>
                • <strong>Key (K):</strong> "What do I offer?"<br>
                • <strong>Value (V):</strong> "What's my actual content?"<br><br>
                Attention = softmax(Q × K^T) × V<br><br>
                <strong>Result:</strong> Each token gets updated based on ALL other tokens!
            </div>
            
            <div class="attention-demo">
                <div class="attention-card">
                    <div class="attention-title">🚀 Parallel Training</div>
                    <div class="attention-value">100%</div>
                    <div>All tokens processed simultaneously during training</div>
                </div>
                
                <div class="attention-card">
                    <div class="attention-title">🎯 Long Dependencies</div>
                    <div class="attention-value">Perfect</div>
                    <div>Direct connections between any tokens</div>
                </div>
                
                <div class="attention-card">
                    <div class="attention-title">📈 Training Speed</div>
                    <div class="attention-value">10-100x</div>
                    <div>Faster training than RNNs</div>
                </div>
                
                <div class="attention-card">
                    <div class="attention-title">🧠 Gradient Flow</div>
                    <div class="attention-value">Perfect</div>
                    <div>No vanishing gradient problem</div>
                </div>
            </div>
        </div>
    </div>

    <div class="container">
        <h2>🏗️ The Architecture: How Transformers Work</h2>
        
        <div class="step">
            <h3>📋 Core Components</h3>
            
            <div class="info">
                <strong>🎯 Key Insight:</strong> Transformers are surprisingly simple - just a few key components stacked together. The magic is in how they combine!
            </div>
            
            <div class="architecture-flow" id="architectureFlow">
                <div class="flow-box" onclick="explainComponent('input')">📝 Input Tokens</div>
                <div class="flow-arrow">↓</div>
                <div class="flow-box" onclick="explainComponent('embedding')">📚 Token Embeddings</div>
                <div class="flow-arrow">↓</div>
                <div class="flow-box" onclick="explainComponent('position')">📍 + Position Encoding</div>
                <div class="flow-arrow">↓</div>
                <div class="flow-box" onclick="explainComponent('attention')">👁️ Self-Attention</div>
                <div class="flow-arrow">↓</div>
                <div class="flow-box" onclick="explainComponent('ffn')">⚡ Feed-Forward Network</div>
                <div class="flow-arrow">↓</div>
                <div class="flow-box" onclick="explainComponent('repeat')">🔄 Repeat N Times</div>
                <div class="flow-arrow">↓</div>
                <div class="flow-box" onclick="explainComponent('output')">🎯 Output Predictions</div>
            </div>
            
            <div id="componentExplanation"></div>
        </div>
        
        <div class="step">
            <h3>🔍 Interactive Architecture Explorer</h3>
            
            <div class="controls">
                <div class="control-group">
                    <label><strong>Architecture Type:</strong></label>
                    <select id="archType">
                        <option value="encoder-decoder">Original Transformer (Encoder-Decoder)</option>
                        <option value="decoder" selected>GPT-style (Decoder-Only)</option>
                        <option value="encoder">BERT-style (Encoder-Only)</option>
                    </select>
                </div>
                <div class="control-group">
                    <label><strong>Model Size:</strong></label>
                    <select id="modelSize">
                        <option value="small">Small (6 layers)</option>
                        <option value="base" selected>Base (12 layers)</option>
                        <option value="large">Large (24 layers)</option>
                    </select>
                </div>
            </div>
            
            <button onclick="exploreArchitecture()">🔍 Explore Architecture Details</button>
            <div id="architectureDetails"></div>
        </div>
    </div>

    <div class="container">
        <h2>🎯 Three Paradigms: Understanding the Transformer Family</h2>
        
        <div class="step">
            <h3>🏛️ The Three Pillars of Modern AI</h3>
            
            <div class="paradigm-comparison">
                <div class="paradigm-card" onclick="selectParadigm('encoder')">
                    <div class="paradigm-title">🔍 Encoder-Only (BERT)</div>
                    <div><strong>Purpose:</strong> Understanding & Classification</div>
                    <div><strong>Attention:</strong> Bidirectional (sees all tokens)</div>
                    <div><strong>Use Cases:</strong> Search, Q&A, Classification</div>
                    <div class="paradigm-details">
                        Input: "The cat sat on the mat"<br>
                        Output: [CLS] vector for classification<br>
                        Every token sees every other token<br>
                        Perfect for understanding tasks
                    </div>
                </div>
                
                <div class="paradigm-card" onclick="selectParadigm('decoder')">
                    <div class="paradigm-title">🚀 Decoder-Only (GPT)</div>
                    <div><strong>Purpose:</strong> Text Generation</div>
                    <div><strong>Attention:</strong> Causal (only sees previous tokens)</div>
                    <div><strong>Use Cases:</strong> ChatGPT, Code Generation, Writing</div>
                    <div class="paradigm-details">
                        Input: "The cat sat on the"<br>
                        Output: "mat" (next token prediction)<br>
                        Autoregressive generation<br>
                        Powers modern chatbots
                    </div>
                </div>
                
                <div class="paradigm-card" onclick="selectParadigm('encdec')">
                    <div class="paradigm-title">🔄 Encoder-Decoder (T5)</div>
                    <div><strong>Purpose:</strong> Sequence-to-Sequence</div>
                    <div><strong>Attention:</strong> Cross-attention between sequences</div>
                    <div><strong>Use Cases:</strong> Translation, Summarization</div>
                    <div class="paradigm-details">
                        Input: "Hello world" (English)<br>
                        Output: "Hola mundo" (Spanish)<br>
                        Encoder understands, decoder generates<br>
                        Best for translation tasks
                    </div>
                </div>
            </div>
            
            <div id="paradigmDetails"></div>
        </div>
    </div>

    <div class="container">
        <h2>📈 The Evolution: From 2017 to Modern AI</h2>
        
        <div class="model-timeline">
            <div class="timeline-item" onclick="showEvolution('transformer')">
                <div class="timeline-year">2017</div>
                <div class="timeline-model">Original Transformer</div>
                <div class="timeline-desc">"Attention is All You Need"</div>
            </div>
            <div class="timeline-item" onclick="showEvolution('bert')">
                <div class="timeline-year">2018</div>
                <div class="timeline-model">BERT</div>
                <div class="timeline-desc">Bidirectional encoder</div>
            </div>
            <div class="timeline-item" onclick="showEvolution('gpt1')">
                <div class="timeline-year">2018</div>
                <div class="timeline-model">GPT-1</div>
                <div class="timeline-desc">Generative pre-training</div>
            </div>
            <div class="timeline-item" onclick="showEvolution('gpt2')">
                <div class="timeline-year">2019</div>
                <div class="timeline-model">GPT-2</div>
                <div class="timeline-desc">Scaling up (1.5B params)</div>
            </div>
            <div class="timeline-item" onclick="showEvolution('gpt3')">
                <div class="timeline-year">2020</div>
                <div class="timeline-model">GPT-3</div>
                <div class="timeline-desc">175B parameters</div>
            </div>
            <div class="timeline-item" onclick="showEvolution('chatgpt')">
                <div class="timeline-year">2022</div>
                <div class="timeline-model">ChatGPT</div>
                <div class="timeline-desc">Mainstream breakthrough</div>
            </div>
        </div>
        
        <div id="evolutionDetails"></div>
        
        <div class="success">
            <strong>🌟 The Revolution:</strong> From a 2017 research paper to powering ChatGPT, Claude, Gemini, and virtually every AI system you use today. Transformers didn't just improve NLP - they created the AI revolution.
        </div>
    </div>

    <div class="container">
        <h2>🎮 Interactive Learning: Why Transformers Win</h2>
        
        <div class="step">
            <h3>🎯 Hands-On Comparison</h3>
            
            <div class="controls">
                <div class="control-group">
                    <label><strong>Sequence Length:</strong></label>
                    <select id="seqLength">
                        <option value="10">10 tokens</option>
                        <option value="100" selected>100 tokens</option>
                        <option value="1000">1000 tokens</option>
                    </select>
                </div>
                <div class="control-group">
                    <label><strong>Comparison Metric:</strong></label>
                    <select id="comparisonMetric">
                        <option value="speed">Processing Speed</option>
                        <option value="memory" selected>Memory Efficiency</option>
                        <option value="dependencies">Long Dependencies</option>
                    </select>
                </div>
            </div>
            
            <button onclick="runComparison()">⚡ Compare Architectures</button>
            <div id="comparisonResults"></div>
        </div>
        
        <div class="step">
            <h3>🔮 What Makes Transformers Special</h3>
            
            <div class="warning">
                <strong>🎯 The Secret Sauce:</strong> It's not just one thing - it's the combination of parallel processing, perfect memory, and scalability that makes transformers so powerful.
            </div>
            
            <button onclick="showTransformerMagic()">✨ Reveal the Transformer Magic</button>
            <div id="transformerMagic"></div>
        </div>
    </div>

    <script>
        let selectedToken = null;
        let selectedParadigm = null;

        function showAttention(tokenIndex) {
            const tokens = ['The', 'cat', 'sat', 'on', 'the', 'mat'];
            const token = tokens[tokenIndex];
            
            // Simulate attention weights (what each token attends to)
            const attentionPatterns = {
                0: [0.1, 0.2, 0.1, 0.1, 0.4, 0.1], // "The" attends most to "the" (determiner pattern)
                1: [0.2, 0.3, 0.3, 0.1, 0.05, 0.05], // "cat" attends to "The" and "sat" 
                2: [0.1, 0.4, 0.2, 0.2, 0.05, 0.05], // "sat" attends most to "cat" (subject-verb)
                3: [0.05, 0.1, 0.2, 0.2, 0.2, 0.25], // "on" attends to "the mat" (prepositional phrase)
                4: [0.3, 0.1, 0.1, 0.2, 0.1, 0.2], // "the" attends to "The" and "mat"
                5: [0.1, 0.2, 0.1, 0.3, 0.2, 0.1]  // "mat" attends to "on" and other tokens
            };
            
            // Update visual selection
            document.querySelectorAll('.token-box').forEach((box, idx) => {
                box.classList.remove('selected');
                if (idx === tokenIndex) box.classList.add('selected');
            });
            
            const weights = attentionPatterns[tokenIndex];
            let html = '<div class="step"><h4>🎯 "' + token + '" Attention Pattern</h4>';
            
            html += '<div class="attention-matrix">';
            for (let i = 0; i < 6; i++) {
                const intensity = Math.floor(weights[i] * 255);
                const color = `rgba(40, 167, 69, ${weights[i]})`;
                html += '<div class="attention-cell" style="background: ' + color + '">';
                html += (weights[i] * 100).toFixed(0) + '%';
                html += '</div>';
            }
            html += '</div>';
            
            html += '<div class="example-box">';
            html += '<strong>"' + token + '" pays attention to:</strong><br>';
            weights.forEach((weight, idx) => {
                if (weight > 0.15) {
                    html += '• <strong>' + tokens[idx] + '</strong>: ' + (weight * 100).toFixed(0) + '% attention<br>';
                }
            });
            html += '</div>';
            
            html += '<div class="info">';
            html += '<strong>🧠 Pattern Recognition:</strong> ';
            if (tokenIndex === 1) {
                html += '"cat" attends strongly to "The" (its determiner) and "sat" (the action it performs). This shows the model learning grammatical relationships!';
            } else if (tokenIndex === 2) {
                html += '"sat" attends most to "cat" (the subject performing the action). This demonstrates subject-verb attention patterns.';
            } else if (tokenIndex === 3) {
                html += '"on" attends to "the mat" (the prepositional object). This shows how prepositions bind to their objects.';
            } else {
                html += 'Each token learns to attend to contextually relevant words, building understanding of syntax and semantics.';
            }
            html += '</div>';
            
            html += '</div>';
            document.getElementById('attentionResult').innerHTML = html;
        }

        function explainComponent(component) {
            // Remove highlighting from all boxes
            document.querySelectorAll('.flow-box').forEach(box => {
                box.classList.remove('highlight');
            });
            
            // Highlight selected component
            event.target.classList.add('highlight');
            
            let html = '<div class="step"><h4>🔍 Component Deep Dive</h4>';
            
            switch(component) {
                case 'input':
                    html += '<h5>📝 Input Tokens</h5>';
                    html += '<div class="example-box">';
                    html += 'Raw text: "The cat sat"<br>';
                    html += 'Tokenized: ["The", "cat", "sat"]<br>';
                    html += 'Token IDs: [1832, 2420, 3275]<br>';
                    html += 'Each word becomes a number the model can process';
                    html += '</div>';
                    break;
                    
                case 'embedding':
                    html += '<h5>📚 Token Embeddings</h5>';
                    html += '<div class="example-box">';
                    html += 'Convert token IDs to dense vectors:<br>';
                    html += '"The" (ID: 1832) → [0.2, -0.1, 0.8, ...] (768 dimensions)<br>';
                    html += '"cat" (ID: 2420) → [0.5, 0.3, -0.2, ...] (768 dimensions)<br>';
                    html += 'Similar words get similar vectors through training';
                    html += '</div>';
                    break;
                    
                case 'position':
                    html += '<h5>📍 Position Encoding</h5>';
                    html += '<div class="example-box">';
                    html += 'Add positional information:<br>';
                    html += 'Position 0: [sin(0), cos(0), sin(0), cos(0), ...]<br>';
                    html += 'Position 1: [sin(1), cos(1), sin(1), cos(1), ...]<br>';
                    html += 'Without this, "cat sat" = "sat cat" to the model!';
                    html += '</div>';
                    break;
                    
                case 'attention':
                    html += '<h5>👁️ Self-Attention</h5>';
                    html += '<div class="example-box">';
                    html += 'Each token attends to all others:<br>';
                    html += '"cat" looks at: "The" (20%), "cat" (30%), "sat" (40%), ...<br>';
                    html += 'Creates rich contextual representations<br>';
                    html += 'The core innovation that makes transformers work!';
                    html += '</div>';
                    break;
                    
                case 'ffn':
                    html += '<h5>⚡ Feed-Forward Network</h5>';
                    html += '<div class="example-box">';
                    html += 'Process each token independently:<br>';
                    html += 'Input: [768] → Linear → ReLU → Linear → [768]<br>';
                    html += 'Adds non-linearity and processing capacity<br>';
                    html += 'Often 4x larger than attention (3072 hidden units)';
                    html += '</div>';
                    break;
                    
                case 'repeat':
                    html += '<h5>🔄 Layer Stacking</h5>';
                    html += '<div class="example-box">';
                    html += 'Repeat attention + FFN many times:<br>';
                    html += 'GPT-2: 12 layers<br>';
                    html += 'GPT-3: 96 layers<br>';
                    html += 'Each layer refines the representation further';
                    html += '</div>';
                    break;
                    
                case 'output':
                    html += '<h5>🎯 Output Predictions</h5>';
                    html += '<div class="example-box">';
                    html += 'Final layer outputs probabilities:<br>';
                    html += '"The cat sat" → next token probabilities<br>';
                    html += '"on": 40%, "down": 25%, "up": 15%, ...<br>';
                    html += 'Sample from distribution to generate text';
                    html += '</div>';
                    break;
            }
            
            html += '</div>';
            document.getElementById('componentExplanation').innerHTML = html;
        }

        function exploreArchitecture() {
            const archType = document.getElementById('archType').value;
            const modelSize = document.getElementById('modelSize').value;
            
            const layerCounts = { small: 6, base: 12, large: 24 };
            const layers = layerCounts[modelSize];
            
            let html = '<div class="step"><h3>🏗️ ' + archType.replace('-', ' ').toUpperCase() + ' Architecture</h3>';
            
            if (archType === 'decoder') {
                html += '<div class="example-box">';
                html += '<strong>GPT-Style (Decoder-Only):</strong><br>';
                html += '• ' + layers + ' transformer layers<br>';
                html += '• Causal attention (can only see previous tokens)<br>';
                html += '• Autoregressive generation (one token at a time)<br>';
                html += '• Powers: ChatGPT, Claude, LLaMA, Gemini<br>';
                html += '• Use case: Text generation, chatbots, coding assistants';
                html += '</div>';
                
                html += '<div class="math-formula">';
                html += '<strong>Causal Attention Mask:</strong><br>';
                html += '1 0 0 0  ← "The" can only see itself<br>';
                html += '1 1 0 0  ← "cat" can see "The" and itself<br>';
                html += '1 1 1 0  ← "sat" can see "The", "cat", itself<br>';
                html += '1 1 1 1  ← "on" can see all previous tokens';
                html += '</div>';
                
            } else if (archType === 'encoder') {
                html += '<div class="example-box">';
                html += '<strong>BERT-Style (Encoder-Only):</strong><br>';
                html += '• ' + layers + ' transformer layers<br>';
                html += '• Bidirectional attention (can see all tokens)<br>';
                html += '• Perfect for understanding tasks<br>';
                html += '• Powers: Search engines, Q&A systems, classification<br>';
                html += '• Use case: Document understanding, semantic search';
                html += '</div>';
                
                html += '<div class="math-formula">';
                html += '<strong>Bidirectional Attention Mask:</strong><br>';
                html += '1 1 1 1  ← Every token can see<br>';
                html += '1 1 1 1  ← all other tokens<br>';
                html += '1 1 1 1  ← Perfect for understanding<br>';
                html += '1 1 1 1  ← but can\'t generate';
                html += '</div>';
                
            } else {
                html += '<div class="example-box">';
                html += '<strong>Original Transformer (Encoder-Decoder):</strong><br>';
                html += '• Encoder: ' + layers + ' layers (bidirectional)<br>';
                html += '• Decoder: ' + layers + ' layers (causal)<br>';
                html += '• Cross-attention between encoder and decoder<br>';
                html += '• Powers: Translation systems, T5, summarization<br>';
                html += '• Use case: Translation, summarization, seq2seq tasks';
                html += '</div>';
                
                html += '<div class="math-formula">';
                html += '<strong>Cross-Attention Flow:</strong><br>';
                html += 'Encoder: "Hello world" → representations<br>';
                html += 'Decoder: "Hola" → attends to encoder representations<br>';
                html += 'Result: "mundo" (next Spanish token)';
                html += '</div>';
            }
            
            html += '</div>';
            document.getElementById('architectureDetails').innerHTML = html;
        }

        function selectParadigm(paradigm) {
            selectedParadigm = paradigm;
            
            // Update visual selection
            document.querySelectorAll('.paradigm-card').forEach(card => {
                card.classList.remove('selected');
            });
            event.target.classList.add('selected');
            
            let html = '<div class="step"><h3>🎯 ' + paradigm.toUpperCase() + ' Deep Dive</h3>';
            
            if (paradigm === 'encoder') {
                html += '<div class="info">';
                html += '<strong>🔍 BERT-Style Encoders Excel At:</strong><br>';
                html += '• Understanding context from both directions<br>';
                html += '• Classification tasks (sentiment, topic, etc.)<br>';
                html += '• Question answering<br>';
                html += '• Semantic search and embeddings<br>';
                html += '• Any task where you need to understand, not generate';
                html += '</div>';
                
            } else if (paradigm === 'decoder') {
                html += '<div class="info">';
                html += '<strong>🚀 GPT-Style Decoders Excel At:</strong><br>';
                html += '• Text generation and completion<br>';
                html += '• Conversational AI (ChatGPT, Claude)<br>';
                html += '• Code generation and programming help<br>';
                html += '• Creative writing and storytelling<br>';
                html += '• Any task where you need to generate new content';
                html += '</div>';
                
            } else {
                html += '<div class="info">';
                html += '<strong>🔄 Encoder-Decoder Models Excel At:</strong><br>';
                html += '• Language translation<br>';
                html += '• Summarization (long text → short summary)<br>';
                html += '• Any sequence-to-sequence transformation<br>';
                html += '• Tasks where input and output are different lengths<br>';
                html += '• Classic NLP tasks like machine translation';
                html += '</div>';
            }
            
            html += '</div>';
            document.getElementById('paradigmDetails').innerHTML = html;
        }

        function showEvolution(model) {
            let html = '<div class="step"><h3>📈 ' + model.toUpperCase() + ' Revolution</h3>';
            
            switch(model) {
                case 'transformer':
                    html += '<div class="example-box">';
                    html += '<strong>Original Transformer (2017):</strong><br>';
                    html += '• Introduced attention mechanism<br>';
                    html += '• Encoder-decoder architecture<br>';
                    html += '• Designed for machine translation<br>';
                    html += '• 65M parameters (tiny by today\'s standards)<br>';
                    html += '• Proved attention could replace recurrence entirely';
                    html += '</div>';
                    break;
                    
                case 'bert':
                    html += '<div class="example-box">';
                    html += '<strong>BERT (2018):</strong><br>';
                    html += '• Bidirectional encoder (revolutionary for understanding)<br>';
                    html += '• Masked language modeling training<br>';
                    html += '• 110M-340M parameters<br>';
                    html += '• Dominated NLP benchmarks overnight<br>';
                    html += '• Showed pre-training + fine-tuning paradigm';
                    html += '</div>';
                    break;
                    
                case 'gpt1':
                    html += '<div class="example-box">';
                    html += '<strong>GPT-1 (2018):</strong><br>';
                    html += '• First decoder-only transformer<br>';
                    html += '• Generative pre-training approach<br>';
                    html += '• 117M parameters<br>';
                    html += '• Showed unsupervised pre-training works<br>';
                    html += '• Foundation for all modern LLMs';
                    html += '</div>';
                    break;
                    
                case 'gpt2':
                    html += '<div class="example-box">';
                    html += '<strong>GPT-2 (2019):</strong><br>';
                    html += '• Scaled up to 1.5B parameters<br>';
                    html += '• "Dangerous to release" (initially withheld)<br>';
                    html += '• Showed coherent long-form generation<br>';
                    html += '• Zero-shot task performance<br>';
                    html += '• Proved scaling laws begin to emerge';
                    html += '</div>';
                    break;
                    
                case 'gpt3':
                    html += '<div class="example-box">';
                    html += '<strong>GPT-3 (2020):</strong><br>';
                    html += '• Massive scale: 175B parameters<br>';
                    html += '• Few-shot learning without fine-tuning<br>';
                    html += '• Emergent abilities (reasoning, code, etc.)<br>';
                    html += '• Foundation for ChatGPT<br>';
                    html += '• Demonstrated true AI capabilities';
                    html += '</div>';
                    break;
                    
                case 'chatgpt':
                    html += '<div class="example-box">';
                    html += '<strong>ChatGPT (2022):</strong><br>';
                    html += '• GPT-3.5 with RLHF (human feedback)<br>';
                    html += '• Conversational interface<br>';
                    html += '• 100M users in 2 months<br>';
                    html += '• Mainstream AI breakthrough<br>';
                    html += '• Launched the current AI revolution';
                    html += '</div>';
                    break;
            }
            
            html += '</div>';
            document.getElementById('evolutionDetails').innerHTML = html;
        }

        function runComparison() {
            const seqLen = parseInt(document.getElementById('seqLength').value);
            const metric = document.getElementById('comparisonMetric').value;
            
            let html = '<div class="step"><h3>⚡ Architecture Comparison Results</h3>';
            
            if (metric === 'speed') {
                const rnnTime = seqLen; // Sequential
                const cnnTime = Math.log(seqLen); // Parallel but limited
                const transformerTime = 1; // Fully parallel
                
                html += '<table>';
                html += '<tr><th>Architecture</th><th>Processing Time</th><th>Parallelization</th><th>Winner</th></tr>';
                html += '<tr><td>RNN/LSTM</td><td>' + rnnTime + ' steps</td><td>❌ Sequential</td><td></td></tr>';
                html += '<tr><td>CNN</td><td>' + cnnTime.toFixed(1) + ' steps</td><td>✅ Partial</td><td></td></tr>';
                html += '<tr class="comparison-winner"><td>Transformer</td><td>' + transformerTime + ' step</td><td>✅ Full</td><td>🏆</td></tr>';
                html += '</table>';
                
                html += '<div class="success">';
                html += '<strong>🚀 Training Speed Winner: Transformer</strong><br>';
                html += 'During training, processes all ' + seqLen + ' tokens simultaneously while RNNs need ' + seqLen + ' sequential steps! Note: During inference (generation), transformers are still autoregressive (one token at a time).';
                html += '</div>';
                
            } else if (metric === 'memory') {
                const rnnMemory = seqLen; // Linear in sequence length
                const cnnMemory = seqLen * 3; // Kernel windows
                const transformerMemory = seqLen * seqLen; // Attention matrix
                
                html += '<table>';
                html += '<tr><th>Architecture</th><th>Memory Usage</th><th>Scaling</th><th>Winner</th></tr>';
                html += '<tr class="comparison-winner"><td>RNN/LSTM</td><td>' + rnnMemory + ' units</td><td>O(n)</td><td>🏆</td></tr>';
                html += '<tr><td>CNN</td><td>' + cnnMemory + ' units</td><td>O(n)</td><td></td></tr>';
                html += '<tr class="comparison-loser"><td>Transformer</td><td>' + (transformerMemory/100).toFixed(0) + '00 units</td><td>O(n²)</td><td>❌</td></tr>';
                html += '</table>';
                
                html += '<div class="warning">';
                html += '<strong>📊 Memory Trade-off:</strong><br>';
                html += 'Transformers use more memory (quadratic) but this enables their superior capabilities. Modern optimizations (like attention evolution tutorials) address this!';
                html += '</div>';
                
            } else {
                html += '<table>';
                html += '<tr><th>Architecture</th><th>Max Dependency Range</th><th>Information Loss</th><th>Winner</th></tr>';
                html += '<tr class="comparison-loser"><td>RNN/LSTM</td><td>~10-50 tokens</td><td>❌ Vanishing gradients</td><td></td></tr>';
                html += '<tr class="comparison-loser"><td>CNN</td><td>Kernel size × layers</td><td>❌ Limited receptive field</td><td></td></tr>';
                html += '<tr class="comparison-winner"><td>Transformer</td><td>Unlimited</td><td>✅ Perfect memory</td><td>🏆</td></tr>';
                html += '</table>';
                
                html += '<div class="success">';
                html += '<strong>🎯 Long Dependencies Winner: Transformer</strong><br>';
                html += 'Every token can directly attend to every other token, no matter how far apart! This enables understanding of complex, long-range relationships.';
                html += '</div>';
            }
            
            html += '</div>';
            document.getElementById('comparisonResults').innerHTML = html;
        }

        function showTransformerMagic() {
            let html = '<div class="step"><h3>✨ The Transformer Magic Formula</h3>';
            
            html += '<div class="attention-demo">';
            html += '<div class="attention-card">';
            html += '<div class="attention-title">🚀 Parallel Processing</div>';
            html += '<div>All tokens processed simultaneously</div>';
            html += '</div>';
            html += '<div class="attention-card">';
            html += '<div class="attention-title">🧠 Perfect Memory</div>';
            html += '<div>Direct connections between any tokens</div>';
            html += '</div>';
            html += '<div class="attention-card">';
            html += '<div class="attention-title">📈 Infinite Scalability</div>';
            html += '<div>More layers = more capabilities</div>';
            html += '</div>';
            html += '<div class="attention-card">';
            html += '<div class="attention-title">🎯 Universal Architecture</div>';
            html += '<div>Same design for all language tasks</div>';
            html += '</div>';
            html += '</div>';
            
            html += '<div class="math-formula">';
            html += '<strong>Training vs Inference Distinction:</strong><br><br>';
            html += '<strong>Training (Parallel):</strong><br>';
            html += '• Input: "The cat sat on the mat" (full sequence)<br>';
            html += '• Simultaneously predict: "cat"|"sat"|"on"|"the"|"mat" (all next tokens)<br>';
            html += '• All predictions computed in parallel<br>';
            html += '• Enables fast training on large datasets<br><br>';
            html += '<strong>Inference (Sequential):</strong><br>';
            html += '• Input: "The cat sat on the"<br>';
            html += '• Predict: "mat" (one token at a time)<br>';
            html += '• Append and repeat: "The cat sat on the mat"<br>';
            html += '• Autoregressive generation (still sequential)';
            html += '</div>';
            
            html += '<div class="success">';
            html += '<strong>🌟 Why Transformers Won:</strong> They solved the three fundamental challenges of sequence modeling: parallelization (speed), long-range dependencies (memory), and scalability (size). Every other architecture failed at least one of these!';
            html += '</div>';
            
            html += '</div>';
            document.getElementById('transformerMagic').innerHTML = html;
        }

        // Initialize with first attention demo
        document.addEventListener('DOMContentLoaded', function() {
            showAttention(1); // Start with "cat" token
            exploreArchitecture(); // Show default architecture
        });
    </script>
</body>
</html>
