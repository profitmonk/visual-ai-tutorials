<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Architecture Comparison: Modern LLM Designs</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background: linear-gradient(135deg, #1a1a1a 0%, #2d2d2d 100%);
            color: #e0e0e0;
            line-height: 1.6;
        }
        
        .container {
            background: #ffffff;
            color: #2d2d2d;
            border-radius: 20px;
            padding: 30px;
            margin: 20px 0;
            border: 1px solid #e0e0e0;
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
        }
        
        .nav-bar {
            background: #2d2d2d;
            color: white;
            padding: 15px 30px;
            border-radius: 15px;
            margin: 20px 0;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }
        
        .nav-home {
            background: #ffffff;
            color: #2d2d2d;
            padding: 8px 16px;
            border-radius: 6px;
            text-decoration: none;
            font-weight: bold;
            transition: all 0.3s ease;
        }
        
        .nav-home:hover {
            background: #f8f9fa;
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.2);
        }
        
        .nav-title {
            font-size: 1.2em;
            font-weight: bold;
        }
        
        .step {
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            padding: 20px;
            margin: 15px 0;
            border-radius: 15px;
            border-left: 4px solid #2d2d2d;
        }
        
        .example-box {
            background: #2d2d2d;
            color: #e0e0e0;
            padding: 15px;
            border-radius: 10px;
            margin: 10px 0;
            font-family: 'Courier New', monospace;
            overflow-x: auto;
            border: 1px solid #4a4a4a;
        }
        
        .controls {
            display: flex;
            gap: 15px;
            margin: 20px 0;
            flex-wrap: wrap;
        }
        
        .control-group {
            display: flex;
            flex-direction: column;
            gap: 5px;
        }
        
        button {
            background: #2d2d2d;
            border: none;
            color: white;
            padding: 12px 24px;
            border-radius: 8px;
            cursor: pointer;
            font-weight: bold;
            transition: all 0.3s ease;
        }
        
        button:hover {
            background: #1a1a1a;
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.2);
        }
        
        input, select {
            padding: 8px 12px;
            border: 1px solid #dadce0;
            border-radius: 6px;
            background: #ffffff;
            color: #2d2d2d;
        }
        
        .warning {
            background: #fff3cd;
            border-left: 4px solid #ffc107;
            color: #856404;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
        }
        
        .success {
            background: #d4edda;
            border-left: 4px solid #28a745;
            color: #155724;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
        }
        
        .info {
            background: #d1ecf1;
            border-left: 4px solid #17a2b8;
            color: #0c5460;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
        }
        
        .danger {
            background: #f8d7da;
            border-left: 4px solid #dc3545;
            color: #721c24;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 15px 0;
            background: #ffffff;
            border: 1px solid #e9ecef;
            border-radius: 8px;
            overflow: hidden;
        }
        
        th {
            background: #2d2d2d;
            color: white;
            padding: 12px;
            text-align: center;
            font-weight: bold;
        }
        
        td {
            padding: 12px;
            text-align: center;
            border-bottom: 1px solid #e9ecef;
        }
        
        .winner {
            background: #d4edda;
            font-weight: bold;
        }
        
        .moderate {
            background: #fff3cd;
        }
        
        .poor {
            background: #f8d7da;
        }
        
        .math-formula {
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            padding: 15px;
            border-radius: 8px;
            margin: 10px 0;
            font-family: 'Courier New', monospace;
            text-align: center;
            font-size: 14px;
        }
        
        .architecture-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }
        
        .arch-card {
            background: #f8f9fa;
            border: 2px solid #e9ecef;
            border-radius: 12px;
            padding: 20px;
            text-align: center;
            cursor: pointer;
            transition: all 0.3s ease;
        }
        
        .arch-card:hover {
            border-color: #2d2d2d;
            transform: translateY(-5px);
            box-shadow: 0 8px 25px rgba(0, 0, 0, 0.15);
        }
        
        .arch-card.selected {
            border-color: #28a745;
            background: #d4edda;
        }
        
        .arch-title {
            font-size: 1.2em;
            font-weight: bold;
            margin-bottom: 10px;
            color: #2d2d2d;
        }
        
        .arch-specs {
            background: #2d2d2d;
            color: #e0e0e0;
            padding: 10px;
            border-radius: 6px;
            font-family: 'Courier New', monospace;
            font-size: 12px;
            margin-top: 10px;
        }
        
        .innovation-badge {
            background: #dc3545;
            color: white;
            padding: 4px 8px;
            border-radius: 12px;
            font-size: 10px;
            font-weight: bold;
            margin: 2px;
            display: inline-block;
        }
        
        .efficiency-badge {
            background: #28a745;
            color: white;
            padding: 4px 8px;
            border-radius: 12px;
            font-size: 10px;
            font-weight: bold;
            margin: 2px;
            display: inline-block;
        }
        
        .comparison-matrix {
            overflow-x: auto;
            margin: 20px 0;
        }
        
        .feature-comparison {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 20px 0;
        }
        
        .feature-card {
            background: #f8f9fa;
            border: 2px solid #e9ecef;
            border-radius: 12px;
            padding: 20px;
        }
        
        .feature-title {
            font-size: 1.1em;
            font-weight: bold;
            margin-bottom: 15px;
            color: #2d2d2d;
        }
        
        .vs-divider {
            text-align: center;
            font-size: 2em;
            font-weight: bold;
            color: #dc3545;
            margin: 20px 0;
        }
        
        .timeline {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin: 30px 0;
            padding: 20px;
            background: #f8f9fa;
            border-radius: 12px;
            border: 2px solid #e9ecef;
        }
        
        .timeline-item {
            text-align: center;
            flex: 1;
            cursor: pointer;
            transition: all 0.3s ease;
        }
        
        .timeline-item:hover {
            transform: scale(1.1);
        }
        
        .timeline-date {
            background: #2d2d2d;
            color: white;
            padding: 6px 12px;
            border-radius: 15px;
            font-weight: bold;
            margin-bottom: 8px;
            display: inline-block;
            font-size: 12px;
        }
        
        .timeline-model {
            font-weight: bold;
            color: #2d2d2d;
            margin-bottom: 5px;
            font-size: 14px;
        }
        
        .timeline-innovation {
            font-size: 11px;
            color: #666;
        }
    </style>
</head>
<body>
    <div class="nav-bar">
        <div class="nav-title">üìä Architecture Comparison: Modern LLM Designs</div>
        <a href="index.html" class="nav-home">üè† Home</a>
    </div>

    <div class="container">
        <h1>üìä Modern LLM Architecture Comparison</h1>
        <p>Comprehensive analysis of production LLM architectures from industry leaders - understanding design decisions, trade-offs, and innovations that shape 2025's AI landscape</p>
        
        <div class="info">
            <strong>üéØ Based on Sebastian Raschka's research:</strong> This tutorial analyzes real production architectures from DeepSeek, Meta, Google, Mistral, Alibaba, and more - focusing on the architectural innovations that make modern LLMs work.
        </div>
    </div>

    <div class="container">
        <h2>üï∞Ô∏è The Architecture Evolution Timeline</h2>
        
        <div class="timeline">
            <div class="timeline-item" onclick="showEvolution('deepseek')">
                <div class="timeline-date">Dec 2024</div>
                <div class="timeline-model">DeepSeek V3</div>
                <div class="timeline-innovation">MLA + MoE Revolution</div>
            </div>
            <div class="timeline-item" onclick="showEvolution('olmo')">
                <div class="timeline-date">Jan 2025</div>
                <div class="timeline-model">OLMo 2</div>
                <div class="timeline-innovation">Post-Norm + QK-Norm</div>
            </div>
            <div class="timeline-item" onclick="showEvolution('gemma')">
                <div class="timeline-date">Feb 2025</div>
                <div class="timeline-model">Gemma 3</div>
                <div class="timeline-innovation">Sliding Window 5:1</div>
            </div>
            <div class="timeline-item" onclick="showEvolution('llama')">
                <div class="timeline-date">Mar 2025</div>
                <div class="timeline-model">Llama 4</div>
                <div class="timeline-innovation">Alternating MoE</div>
            </div>
            <div class="timeline-item" onclick="showEvolution('kimi')">
                <div class="timeline-date">May 2025</div>
                <div class="timeline-model">Kimi K2</div>
                <div class="timeline-innovation">1T Parameters</div>
            </div>
        </div>
        
        <div id="evolutionDetails"></div>
    </div>

    <div class="container">
        <h2>üèóÔ∏è Interactive Architecture Explorer</h2>
        
        <div class="controls">
            <div class="control-group">
                <label><strong>Primary Model:</strong></label>
                <select id="primaryModel">
                    <option value="deepseek-v3" selected>DeepSeek V3 (671B)</option>
                    <option value="llama-4">Llama 4 Maverick (400B)</option>
                    <option value="gemma-3">Gemma 3 (27B)</option>
                    <option value="qwen3-235b">Qwen3 235B-A22B</option>
                    <option value="kimi-k2">Kimi K2 (1T)</option>
                </select>
            </div>
            <div class="control-group">
                <label><strong>Comparison Model:</strong></label>
                <select id="comparisonModel">
                    <option value="llama-4" selected>Llama 4 Maverick (400B)</option>
                    <option value="deepseek-v3">DeepSeek V3 (671B)</option>
                    <option value="gemma-3">Gemma 3 (27B)</option>
                    <option value="qwen3-235b">Qwen3 235B-A22B</option>
                    <option value="olmo-2">OLMo 2 (13B)</option>
                </select>
            </div>
            <div class="control-group">
                <label><strong>Focus Area:</strong></label>
                <select id="focusArea">
                    <option value="attention" selected>Attention Mechanisms</option>
                    <option value="efficiency">Efficiency Strategies</option>
                    <option value="scaling">Scaling Approaches</option>
                    <option value="normalization">Normalization Techniques</option>
                </select>
            </div>
        </div>
        
        <button onclick="compareArchitectures()">üîç Compare Architectures</button>
        <div id="architectureComparison"></div>
    </div>

    <div class="container">
        <h2>‚ö° DeepSeek V3 vs Llama 4: The MoE Showdown</h2>
        
        <div class="warning">
            <strong>üéØ Key Battle:</strong> Two of 2025's most important architectures take completely different approaches to Mixture of Experts and attention mechanisms.
        </div>
        
        <div class="feature-comparison">
            <div class="feature-card">
                <div class="feature-title">üß† DeepSeek V3 Strategy</div>
                <div class="innovation-badge">MLA</div>
                <div class="efficiency-badge">Dense MoE</div>
                <div class="efficiency-badge">Shared Expert</div>
                
                <div class="example-box">
                    <strong>Architecture Highlights:</strong><br>
                    ‚Ä¢ 671B total parameters, 37B active<br>
                    ‚Ä¢ Multi-Head Latent Attention (MLA)<br>
                    ‚Ä¢ 256 experts, 9 active (8 + 1 shared)<br>
                    ‚Ä¢ MoE in every layer (except first 3)<br>
                    ‚Ä¢ Compressed KV cache via MLA<br>
                    ‚Ä¢ Expert size: 2,048 hidden units
                </div>
                
                <div class="math-formula">
                    <strong>MLA Compression:</strong><br>
                    K, V ‚Üí compress ‚Üí KV cache ‚Üí decompress ‚Üí attention<br>
                    Memory savings: ~50% vs standard GQA
                </div>
            </div>
            
            <div class="feature-card">
                <div class="feature-title">üöÄ Llama 4 Strategy</div>
                <div class="innovation-badge">GQA</div>
                <div class="efficiency-badge">Sparse MoE</div>
                <div class="efficiency-badge">Alternating</div>
                
                <div class="example-box">
                    <strong>Architecture Highlights:</strong><br>
                    ‚Ä¢ 400B total parameters, 17B active<br>
                    ‚Ä¢ Grouped Query Attention (GQA)<br>
                    ‚Ä¢ Fewer experts, 2 active per token<br>
                    ‚Ä¢ Alternates MoE/dense every other layer<br>
                    ‚Ä¢ Standard KV cache approach<br>
                    ‚Ä¢ Expert size: 8,192 hidden units
                </div>
                
                <div class="math-formula">
                    <strong>Alternating MoE:</strong><br>
                    Layer 1: Dense ‚Üí Layer 2: MoE ‚Üí Layer 3: Dense...<br>
                    Fewer active params but larger experts
                </div>
            </div>
        </div>
        
        <div class="vs-divider">VS</div>
        
        <div class="step">
            <h3>üìä Head-to-Head Analysis</h3>
            
            <div class="comparison-matrix">
                <table>
                    <tr>
                        <th>Aspect</th>
                        <th>DeepSeek V3</th>
                        <th>Llama 4 Maverick</th>
                        <th>Winner</th>
                    </tr>
                    <tr>
                        <td><strong>Total Parameters</strong></td>
                        <td>671B</td>
                        <td>400B</td>
                        <td class="winner">DeepSeek (Capacity)</td>
                    </tr>
                    <tr>
                        <td><strong>Active Parameters</strong></td>
                        <td>37B</td>
                        <td>17B</td>
                        <td class="winner">DeepSeek (More Active)</td>
                    </tr>
                    <tr>
                        <td><strong>Attention Type</strong></td>
                        <td>MLA (compressed)</td>
                        <td>GQA (standard)</td>
                        <td class="moderate">Trade-off</td>
                    </tr>
                    <tr>
                        <td><strong>MoE Strategy</strong></td>
                        <td>Dense (every layer)</td>
                        <td>Sparse (alternating)</td>
                        <td class="moderate">Different approaches</td>
                    </tr>
                    <tr>
                        <td><strong>KV Cache Memory</strong></td>
                        <td class="winner">~50% savings (MLA)</td>
                        <td>Standard usage</td>
                        <td class="winner">DeepSeek</td>
                    </tr>
                    <tr>
                        <td><strong>Implementation</strong></td>
                        <td class="poor">Complex (MLA)</td>
                        <td class="winner">Simpler (GQA)</td>
                        <td class="winner">Llama 4</td>
                    </tr>
                </table>
            </div>
            
            <div class="success">
                <strong>üèÜ Key Insight:</strong> DeepSeek V3 optimizes for maximum capability and memory efficiency through MLA, while Llama 4 balances performance with implementation simplicity through proven GQA + alternating MoE.
            </div>
        </div>
        
        <button onclick="showDetailedMoEComparison()">üî¨ Deep Dive: MoE Implementation Details</button>
        <div id="moeDetails"></div>
    </div>

    <div class="container">
        <h2>üéØ Architecture Selection Guide</h2>
        
        <div class="step">
            <h3>ü§î Which Architecture for Which Use Case?</h3>
            
            <div class="architecture-grid">
                <div class="arch-card" onclick="selectUseCase('research')">
                    <div class="arch-title">üî¨ Research & Experimentation</div>
                    <div>Best: OLMo 2, SmolLM3</div>
                    <div class="innovation-badge">Transparent</div>
                    <div class="innovation-badge">Well-documented</div>
                    <div class="arch-specs">
                        ‚Ä¢ Full training details available<br>
                        ‚Ä¢ Clean, standard architectures<br>
                        ‚Ä¢ Educational focus
                    </div>
                </div>
                
                <div class="arch-card" onclick="selectUseCase('production')">
                    <div class="arch-title">üè≠ Production Deployment</div>
                    <div>Best: Llama 4, Gemma 3</div>
                    <div class="efficiency-badge">Proven</div>
                    <div class="efficiency-badge">Optimized</div>
                    <div class="arch-specs">
                        ‚Ä¢ Battle-tested architectures<br>
                        ‚Ä¢ Extensive optimization<br>
                        ‚Ä¢ Ecosystem support
                    </div>
                </div>
                
                <div class="arch-card" onclick="selectUseCase('efficiency')">
                    <div class="arch-title">‚ö° Maximum Efficiency</div>
                    <div>Best: DeepSeek V3, Qwen3</div>
                    <div class="efficiency-badge">MLA/MoE</div>
                    <div class="efficiency-badge">Memory-efficient</div>
                    <div class="arch-specs">
                        ‚Ä¢ Cutting-edge optimizations<br>
                        ‚Ä¢ Lowest inference cost<br>
                        ‚Ä¢ Advanced techniques
                    </div>
                </div>
                
                <div class="arch-card" onclick="selectUseCase('local')">
                    <div class="arch-title">üíª Local Deployment</div>
                    <div>Best: Gemma 3, Qwen3 small</div>
                    <div class="efficiency-badge">Sliding Window</div>
                    <div class="efficiency-badge">Small variants</div>
                    <div class="arch-specs">
                        ‚Ä¢ Reduced memory usage<br>
                        ‚Ä¢ Consumer hardware friendly<br>
                        ‚Ä¢ Multiple size options
                    </div>
                </div>
            </div>
            
            <div id="useCaseDetails"></div>
        </div>
    </div>

    <div class="container">
        <h2>üîÆ Coming Next: Complete Architecture Analysis</h2>
        
        <div class="danger">
            <strong>üöß Tutorial In Progress:</strong> This is Part 1 of the comprehensive architecture comparison. Coming in the next update:
        </div>
        
        <div class="step">
            <h3>üìã Planned Sections (Part 2)</h3>
            <ul>
                <li><strong>üéØ Attention Evolution Deep Dive:</strong> MHA ‚Üí GQA ‚Üí MLA with interactive comparisons</li>
                <li><strong>üìä Normalization Strategies:</strong> Pre-Norm vs Post-Norm vs QK-Norm analysis</li>
                <li><strong>üîÑ MoE Architectures:</strong> Shared experts, routing strategies, load balancing</li>
                <li><strong>‚ö° Efficiency Innovations:</strong> Sliding window attention, NoPE, compression techniques</li>
                <li><strong>üìà Performance Metrics:</strong> Memory usage, inference speed, training efficiency</li>
                <li><strong>üõ†Ô∏è Implementation Complexity:</strong> Engineering trade-offs and deployment considerations</li>
                <li><strong>üî¨ Ablation Studies:</strong> What actually matters in modern architectures</li>
                <li><strong>üéÆ Interactive Architecture Builder:</strong> Mix and match components to design your own LLM</li>
            </ul>
        </div>
        
        <div class="info">
            <strong>üéØ Next Steps:</strong> The complete tutorial will include all 8 architectures from Sebastian Raschka's analysis with full interactive comparisons, mathematical deep dives, and practical deployment guidance.
        </div>
    </div>

    <script>
        // Model specifications database
        const modelSpecs = {
            'deepseek-v3': {
                name: 'DeepSeek V3',
                totalParams: '671B',
                activeParams: '37B',
                attention: 'Multi-Head Latent Attention (MLA)',
                moe: 'Dense MoE (256 experts, 9 active)',
                innovations: ['MLA compression', 'Shared expert', 'KV cache optimization'],
                released: 'December 2024',
                company: 'DeepSeek',
                keyFeature: 'Memory-efficient MLA with massive MoE'
            },
            'llama-4': {
                name: 'Llama 4 Maverick',
                totalParams: '400B',
                activeParams: '17B',
                attention: 'Grouped Query Attention (GQA)',
                moe: 'Alternating MoE (2 active experts)',
                innovations: ['Alternating dense/MoE', 'Large expert size', 'Simplified routing'],
                released: 'March 2025',
                company: 'Meta',
                keyFeature: 'Balanced MoE with proven GQA'
            },
            'gemma-3': {
                name: 'Gemma 3',
                totalParams: '27B',
                activeParams: '27B (dense)',
                attention: 'Sliding Window GQA',
                moe: 'No MoE (dense model)',
                innovations: ['5:1 sliding window', 'Pre+Post norm', 'QK normalization'],
                released: 'February 2025',
                company: 'Google',
                keyFeature: 'Sliding window attention efficiency'
            },
            'qwen3-235b': {
                name: 'Qwen3 235B-A22B',
                totalParams: '235B',
                activeParams: '22B',
                attention: 'Grouped Query Attention (GQA)',
                moe: 'Standard MoE (8 active experts)',
                innovations: ['No shared expert', 'Optimized routing', 'Multi-size variants'],
                released: 'April 2025',
                company: 'Alibaba',
                keyFeature: 'Efficient MoE without shared expert'
            },
            'olmo-2': {
                name: 'OLMo 2',
                totalParams: '13B',
                activeParams: '13B (dense)',
                attention: 'Multi-Head Attention (MHA)',
                moe: 'No MoE (dense model)',
                innovations: ['Post-norm placement', 'QK normalization', 'Training transparency'],
                released: 'January 2025',
                company: 'Allen Institute',
                keyFeature: 'Transparent training with norm innovations'
            },
            'kimi-k2': {
                name: 'Kimi K2',
                totalParams: '1T',
                activeParams: '~100B',
                attention: 'Multi-Head Latent Attention (MLA)',
                moe: 'Massive MoE (like DeepSeek V3)',
                innovations: ['Trillion parameters', 'Muon optimizer', 'Scaled MLA'],
                released: 'May 2025',
                company: 'Moonshot AI',
                keyFeature: 'Largest open-weight model with MLA'
            }
        };

        function showEvolution(model) {
            let html = '<div class="step"><h3>üìà ' + model.toUpperCase() + ' Innovation Spotlight</h3>';
            
            switch(model) {
                case 'deepseek':
                    html += '<div class="example-box">';
                    html += '<strong>DeepSeek V3 Revolution (December 2024):</strong><br>';
                    html += '‚Ä¢ Introduced Multi-Head Latent Attention (MLA) to production<br>';
                    html += '‚Ä¢ Massive 671B parameters with only 37B active<br>';
                    html += '‚Ä¢ 50% KV cache memory savings vs standard GQA<br>';
                    html += '‚Ä¢ Shared expert design for better specialization<br>';
                    html += '‚Ä¢ Outperformed GPT-4 while being more efficient';
                    html += '</div>';
                    break;
                    
                case 'olmo':
                    html += '<div class="example-box">';
                    html += '<strong>OLMo 2 Transparency (January 2025):</strong><br>';
                    html += '‚Ä¢ Returned to Post-Norm for better training stability<br>';
                    html += '‚Ä¢ Added QK-Norm inside attention mechanism<br>';
                    html += '‚Ä¢ Full training transparency - code, data, logs<br>';
                    html += '‚Ä¢ Achieved Pareto frontier on compute/performance<br>';
                    html += '‚Ä¢ Educational blueprint for LLM development';
                    html += '</div>';
                    break;
                    
                case 'gemma':
                    html += '<div class="example-box">';
                    html += '<strong>Gemma 3 Efficiency (February 2025):</strong><br>';
                    html += '‚Ä¢ Revolutionary 5:1 sliding window ratio<br>';
                    html += '‚Ä¢ Reduced window size from 4K to 1K tokens<br>';
                    html += '‚Ä¢ Massive KV cache memory savings<br>';
                    html += '‚Ä¢ Pre+Post normalization for stability<br>';
                    html += '‚Ä¢ Sweet spot 27B parameter size';
                    html += '</div>';
                    break;
                    
                case 'llama':
                    html += '<div class="example-box">';
                    html += '<strong>Llama 4 Balance (March 2025):</strong><br>';
                    html += '‚Ä¢ Alternating MoE/dense layer strategy<br>';
                    html += '‚Ä¢ Proven GQA over experimental MLA<br>';
                    html += '‚Ä¢ Larger experts (8K) with fewer active (2)<br>';
                    html += '‚Ä¢ 400B total with 17B active parameters<br>';
                    html += '‚Ä¢ Production-ready architecture focus';
                    html += '</div>';
                    break;
                    
                case 'kimi':
                    html += '<div class="example-box">';
                    html += '<strong>Kimi K2 Scale (May 2025):</strong><br>';
                    html += '‚Ä¢ First 1 trillion parameter open-weight model<br>';
                    html += '‚Ä¢ Uses DeepSeek V3 architecture at massive scale<br>';
                    html += '‚Ä¢ Muon optimizer over traditional AdamW<br>';
                    html += '‚Ä¢ Smooth training loss curves<br>';
                    html += '‚Ä¢ Matches proprietary model performance';
                    html += '</div>';
                    break;
            }
            
            html += '</div>';
            document.getElementById('evolutionDetails').innerHTML = html;
        }

        function compareArchitectures() {
            const primary = document.getElementById('primaryModel').value;
            const comparison = document.getElementById('comparisonModel').value;
            const focus = document.getElementById('focusArea').value;
            
            const primarySpec = modelSpecs[primary];
            const comparisonSpec = modelSpecs[comparison];
            
            let html = '<div class="step"><h3>üîç ' + primarySpec.name + ' vs ' + comparisonSpec.name + '</h3>';
            
            // Architecture comparison table
            html += '<div class="comparison-matrix">';
            html += '<table>';
            html += '<tr><th>Aspect</th><th>' + primarySpec.name + '</th><th>' + comparisonSpec.name + '</th><th>Analysis</th></tr>';
            html += '<tr><td><strong>Total Parameters</strong></td><td>' + primarySpec.totalParams + '</td><td>' + comparisonSpec.totalParams + '</td><td>Capacity difference</td></tr>';
            html += '<tr><td><strong>Active Parameters</strong></td><td>' + primarySpec.activeParams + '</td><td>' + comparisonSpec.activeParams + '</td><td>Inference efficiency</td></tr>';
            html += '<tr><td><strong>Attention</strong></td><td>' + primarySpec.attention + '</td><td>' + comparisonSpec.attention + '</td><td>Memory/compute trade-off</td></tr>';
            html += '<tr><td><strong>MoE Strategy</strong></td><td>' + primarySpec.moe + '</td><td>' + comparisonSpec.moe + '</td><td>Scaling approach</td></tr>';
            html += '<tr><td><strong>Key Innovation</strong></td><td>' + primarySpec.keyFeature + '</td><td>' + comparisonSpec.keyFeature + '</td><td>Design philosophy</td></tr>';
            html += '</table>';
            html += '</div>';
            
            if (focus === 'attention') {
                html += '<div class="info">';
                html += '<strong>üéØ Attention Focus:</strong> ' + primarySpec.name + ' uses ' + primarySpec.attention + 
                        ' while ' + comparisonSpec.name + ' uses ' + comparisonSpec.attention + '. ';
                        
                if (primarySpec.attention.includes('MLA')) {
                    html += 'MLA provides superior memory efficiency through KV compression but requires more complex implementation.';
                } else if (primarySpec.attention.includes('Sliding')) {
                    html += 'Sliding window attention reduces memory quadratically while maintaining most performance.';
                } else {
                    html += 'Standard GQA provides proven efficiency with simpler implementation.';
                }
                html += '</div>';
            }
            
            html += '</div>';
            document.getElementById('architectureComparison').innerHTML = html;
        }

        function showDetailedMoEComparison() {
            let html = '<div class="step"><h3>üî¨ MoE Implementation Deep Dive</h3>';
            
            html += '<div class="feature-comparison">';
            html += '<div class="feature-card">';
            html += '<div class="feature-title">üß† DeepSeek V3 MoE Design</div>';
            html += '<div class="math-formula">';
            html += '<strong>Expert Configuration:</strong><br>';
            html += '‚Ä¢ 256 total experts per layer<br>';
            html += '‚Ä¢ 9 active: 8 routed + 1 shared<br>';
            html += '‚Ä¢ Expert size: 2,048 hidden units<br>';
            html += '‚Ä¢ Every layer has MoE (except first 3)<br>';
            html += '‚Ä¢ Shared expert always active<br><br>';
            html += '<strong>Routing Math:</strong><br>';
            html += 'token ‚Üí router ‚Üí top-8 experts + shared<br>';
            html += 'output = Œ£(weight_i √ó expert_i(token)) + shared(token)';
            html += '</div>';
            html += '</div>';
            
            html += '<div class="feature-card">';
            html += '<div class="feature-title">üöÄ Llama 4 MoE Design</div>';
            html += '<div class="math-formula">';
            html += '<strong>Expert Configuration:</strong><br>';
            html += '‚Ä¢ Fewer total experts per layer<br>';
            html += '‚Ä¢ 2 active experts per token<br>';
            html += '‚Ä¢ Expert size: 8,192 hidden units<br>';
            html += '‚Ä¢ Alternates: Layer N=MoE, N+1=Dense<br>';
            html += '‚Ä¢ No shared expert<br><br>';
            html += '<strong>Routing Math:</strong><br>';
            html += 'token ‚Üí router ‚Üí top-2 experts<br>';
            html += 'output = weight_1 √ó expert_1(token) + weight_2 √ó expert_2(token)';
            html += '</div>';
            html += '</div>';
            html += '</div>';
            
            html += '<div class="warning">';
            html += '<strong>üéØ Trade-off Analysis:</strong><br>';
            html += '‚Ä¢ <strong>DeepSeek:</strong> More experts = better specialization, higher routing overhead<br>';
            html += '‚Ä¢ <strong>Llama 4:</strong> Fewer experts = simpler routing, larger expert capacity<br>';
            html += '‚Ä¢ <strong>Shared Expert:</strong> Handles common patterns, reduces expert load imbalance<br>';
            html += '‚Ä¢ <strong>Alternating:</strong> Balances dense computation with sparse scaling';
            html += '</div>';
            
            html += '</div>';
            document.getElementById('moeDetails').innerHTML = html;
        }

        function selectUseCase(useCase) {
            // Remove selection from all cards
            document.querySelectorAll('.arch-card').forEach(card => {
                card.classList.remove('selected');
            });
            
            // Add selection to clicked card
            event.target.classList.add('selected');
            
            let html = '<div class="step"><h3>üéØ ' + useCase.toUpperCase() + ' Use Case Recommendations</h3>';
            
            switch(useCase) {
                case 'research':
                    html += '<div class="info">';
                    html += '<strong>üî¨ Research & Experimentation:</strong><br>';
                    html += '‚Ä¢ <strong>OLMo 2:</strong> Complete transparency, training logs, clean architecture<br>';
                    html += '‚Ä¢ <strong>SmolLM3:</strong> Small scale for quick experiments, NoPE innovation<br>';
                    html += '‚Ä¢ <strong>Benefits:</strong> Reproducible results, educational value, modification-friendly<br>';
                    html += '‚Ä¢ <strong>Trade-offs:</strong> Not state-of-the-art performance, smaller scale';
                    html += '</div>';
                    break;
                    
                case 'production':
                    html += '<div class="success">';
                    html += '<strong>üè≠ Production Deployment:</strong><br>';
                    html += '‚Ä¢ <strong>Llama 4:</strong> Battle-tested, extensive ecosystem, balanced architecture<br>';
                    html += '‚Ä¢ <strong>Gemma 3:</strong> Efficient, well-optimized, good documentation<br>';
                    html += '‚Ä¢ <strong>Benefits:</strong> Proven reliability, optimization support, community resources<br>';
                    html += '‚Ä¢ <strong>Trade-offs:</strong> Less cutting-edge, potentially higher inference costs';
                    html += '</div>';
                    break;
                    
                case 'efficiency':
                    html += '<div class="warning">';
                    html += '<strong>‚ö° Maximum Efficiency:</strong><br>';
                    html += '‚Ä¢ <strong>DeepSeek V3:</strong> MLA compression, optimal MoE design, memory efficient<br>';
                    html += '‚Ä¢ <strong>Qwen3:</strong> Multiple variants, optimized routing, good performance/cost<br>';
                    html += '‚Ä¢ <strong>Benefits:</strong> Lowest inference cost, cutting-edge optimizations<br>';
                    html += '‚Ä¢ <strong>Trade-offs:</strong> Implementation complexity, newer/less tested';
                    html += '</div>';
                    break;
                    
                case 'local':
                    html += '<div class="info">';
                    html += '<strong>üíª Local Deployment:</strong><br>';
                    html += '‚Ä¢ <strong>Gemma 3:</strong> Sliding window reduces memory, multiple sizes<br>';
                    html += '‚Ä¢ <strong>Qwen3 Small:</strong> Excellent small models, efficient architecture<br>';
                    html += '‚Ä¢ <strong>Benefits:</strong> Consumer hardware friendly, privacy control<br>';
                    html += '‚Ä¢ <strong>Trade-offs:</strong> Limited by local compute, smaller model capabilities';
                    html += '</div>';
                    break;
            }
            
            html += '</div>';
            document.getElementById('useCaseDetails').innerHTML = html;
        }

        // Initialize with first comparison
        document.addEventListener('DOMContentLoaded', function() {
            compareArchitectures();
            showEvolution('deepseek');
        });
    </script>
</body>
</html>
