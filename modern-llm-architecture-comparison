<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Modern LLM Architecture Comparison</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background: linear-gradient(135deg, #1a1a1a 0%, #2d2d2d 100%);
            color: #e0e0e0;
            line-height: 1.6;
        }
        
        .container {
            background: #ffffff;
            color: #2d2d2d;
            border-radius: 20px;
            padding: 30px;
            margin: 20px 0;
            border: 1px solid #e0e0e0;
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
        }
        
        .nav-bar {
            background: #2d2d2d;
            color: white;
            padding: 15px 30px;
            border-radius: 15px;
            margin: 20px 0;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }
        
        .nav-home {
            background: #ffffff;
            color: #2d2d2d;
            padding: 8px 16px;
            border-radius: 6px;
            text-decoration: none;
            font-weight: bold;
            transition: all 0.3s ease;
        }
        
        .nav-home:hover {
            background: #f8f9fa;
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.2);
        }
        
        .nav-title {
            font-size: 1.2em;
            font-weight: bold;
        }
        
        .step {
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            padding: 20px;
            margin: 15px 0;
            border-radius: 15px;
            border-left: 4px solid #2d2d2d;
        }
        
        .example-box {
            background: #2d2d2d;
            color: #e0e0e0;
            padding: 15px;
            border-radius: 10px;
            margin: 10px 0;
            font-family: 'Courier New', monospace;
            overflow-x: auto;
            border: 1px solid #4a4a4a;
        }
        
        .controls {
            display: flex;
            gap: 15px;
            margin: 20px 0;
            flex-wrap: wrap;
        }
        
        .control-group {
            display: flex;
            flex-direction: column;
            gap: 5px;
        }
        
        button {
            background: #2d2d2d;
            border: none;
            color: white;
            padding: 12px 24px;
            border-radius: 8px;
            cursor: pointer;
            font-weight: bold;
            transition: all 0.3s ease;
        }
        
        button:hover {
            background: #1a1a1a;
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.2);
        }
        
        input, select {
            padding: 8px 12px;
            border: 1px solid #dadce0;
            border-radius: 6px;
            background: #ffffff;
            color: #2d2d2d;
        }
        
        .warning {
            background: #fff3cd;
            border-left: 4px solid #ffc107;
            color: #856404;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
        }
        
        .success {
            background: #d4edda;
            border-left: 4px solid #28a745;
            color: #155724;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
        }
        
        .info {
            background: #d1ecf1;
            border-left: 4px solid #17a2b8;
            color: #0c5460;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
        }
        
        .danger {
            background: #f8d7da;
            border-left: 4px solid #dc3545;
            color: #721c24;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 15px 0;
            background: #ffffff;
            border: 1px solid #e9ecef;
            border-radius: 8px;
            overflow: hidden;
        }
        
        th {
            background: #2d2d2d;
            color: white;
            padding: 12px;
            text-align: center;
            font-weight: bold;
        }
        
        td {
            padding: 12px;
            text-align: center;
            border-bottom: 1px solid #e9ecef;
        }
        
        .model-highlight {
            background: #d4edda;
            font-weight: bold;
        }
        
        .comparison-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }
        
        .model-card {
            background: #f8f9fa;
            border: 2px solid #e9ecef;
            border-radius: 12px;
            padding: 20px;
            transition: all 0.3s ease;
            cursor: pointer;
        }
        
        .model-card:hover {
            border-color: #2d2d2d;
            transform: translateY(-5px);
            box-shadow: 0 8px 25px rgba(0, 0, 0, 0.15);
        }
        
        .model-card.selected {
            border-color: #28a745;
            background: #d4edda;
        }
        
        .model-title {
            font-size: 1.2em;
            font-weight: bold;
            margin-bottom: 10px;
            color: #2d2d2d;
            text-align: center;
        }
        
        .model-specs {
            background: #2d2d2d;
            color: #e0e0e0;
            padding: 15px;
            border-radius: 8px;
            font-family: 'Courier New', monospace;
            font-size: 12px;
            margin-top: 10px;
        }
        
        .architecture-diagram {
            display: flex;
            flex-direction: column;
            align-items: center;
            margin: 30px 0;
            padding: 20px;
            background: #f8f9fa;
            border-radius: 15px;
            border: 2px solid #e9ecef;
        }
        
        .layer-box {
            background: #2d2d2d;
            color: white;
            padding: 12px 20px;
            margin: 5px;
            border-radius: 8px;
            font-weight: bold;
            text-align: center;
            min-width: 200px;
            font-size: 14px;
        }
        
        .layer-highlight {
            background: #28a745;
        }
        
        .layer-difference {
            background: #dc3545;
        }
        
        .arrow-down {
            font-size: 20px;
            color: #2d2d2d;
            margin: 3px 0;
        }
        
        .side-by-side {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 30px;
            margin: 20px 0;
        }
        
        .evolution-timeline {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin: 30px 0;
            padding: 20px;
            background: #f8f9fa;
            border-radius: 12px;
            border: 2px solid #e9ecef;
        }
        
        .timeline-item {
            text-align: center;
            flex: 1;
            position: relative;
            cursor: pointer;
            transition: all 0.3s ease;
            padding: 10px;
        }
        
        .timeline-item:hover {
            transform: scale(1.05);
        }
        
        .timeline-item:not(:last-child)::after {
            content: '→';
            position: absolute;
            right: -15px;
            top: 40%;
            font-size: 20px;
            color: #2d2d2d;
            font-weight: bold;
        }
        
        .timeline-year {
            background: #2d2d2d;
            color: white;
            padding: 4px 8px;
            border-radius: 12px;
            font-weight: bold;
            margin-bottom: 5px;
            display: inline-block;
            font-size: 11px;
        }
        
        .timeline-model {
            font-weight: bold;
            color: #2d2d2d;
            margin-bottom: 3px;
            font-size: 13px;
        }
        
        .timeline-innovation {
            font-size: 10px;
            color: #666;
        }
        
        .innovation-highlight {
            background: #fff3cd;
            border-left: 4px solid #ffc107;
            padding: 10px;
            border-radius: 6px;
            margin: 5px 0;
            font-size: 12px;
        }
        
        .component-comparison {
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 10px;
            padding: 15px;
            margin: 10px 0;
        }
        
        .component-title {
            font-weight: bold;
            color: #2d2d2d;
            margin-bottom: 10px;
        }
        
        .difference-indicator {
            display: inline-block;
            width: 12px;
            height: 12px;
            border-radius: 50%;
            margin-right: 8px;
        }
        
        .same {
            background: #28a745;
        }
        
        .different {
            background: #dc3545;
        }
        
        .similar {
            background: #ffc107;
        }
        
        .math-formula {
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            padding: 15px;
            border-radius: 8px;
            margin: 10px 0;
            font-family: 'Courier New', monospace;
            text-align: center;
            font-size: 14px;
        }
        
        .parameter-breakdown {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 15px;
            margin: 20px 0;
        }
        
        .param-card {
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 8px;
            padding: 15px;
            text-align: center;
        }
        
        .param-title {
            font-weight: bold;
            color: #2d2d2d;
            margin-bottom: 5px;
            font-size: 14px;
        }
        
        .param-value {
            font-size: 1.2em;
            font-weight: bold;
            color: #dc3545;
        }
        
        .param-desc {
            font-size: 12px;
            color: #666;
            margin-top: 5px;
        }
    </style>
</head>
<body>
    <div class="nav-bar">
        <div class="nav-title">📊 Modern LLM Architecture Comparison</div>
        <a href="index.html" class="nav-home">🏠 Home</a>
    </div>

    <div class="container">
        <h1>📊 Modern LLM Architecture Comparison</h1>
        <p>Interactive exploration of how today's leading models differ - from LLaMA to DeepSeek to Qwen. See what's actually changed since 2020!</p>
    </div>

    <div class="container">
        <h2>🎯 Model Selection: Choose Your Comparison</h2>
        
        <div class="info">
            <strong>💡 Sebastian Raschka Style:</strong> Pick 2-3 models to see side-by-side architectural differences with real specifications and practical implications.
        </div>
        
        <div class="comparison-grid">
            <div class="model-card" onclick="selectModel('llama3')">
                <div class="model-title">🦙 LLaMA-3-8B</div>
                <div><strong>Meta's Foundation Model</strong></div>
                <div>The gold standard for open models</div>
                <div class="model-specs">
                    32 layers • 4096 d_model<br>
                    32 heads • 8 KV groups (GQA)<br>
                    RMSNorm • SwiGLU • RoPE<br>
                    128K context • 8.03B params
                </div>
            </div>
            
            <div class="model-card" onclick="selectModel('qwen3')">
                <div class="model-title">🔥 Qwen3-8B</div>
                <div><strong>Alibaba's Powerhouse</strong></div>
                <div>Leading leaderboard performance</div>
                <div class="model-specs">
                    28 layers • 4096 d_model<br>
                    32 heads • 8 KV groups (GQA)<br>
                    RMSNorm • SwiGLU • RoPE<br>
                    128K context • 8.1B params
                </div>
            </div>
            
            <div class="model-card" onclick="selectModel('gemma3')">
                <div class="model-title">💎 Gemma 3-9B</div>
                <div><strong>Google's Efficiency Focus</strong></div>
                <div>Sliding window attention</div>
                <div class="model-specs">
                    42 layers • 3584 d_model<br>
                    16 heads • 16 KV groups (GQA)<br>
                    RMSNorm • GeGLU • RoPE<br>
                    8K + sliding window • 9.0B params
                </div>
            </div>
            
            <div class="model-card" onclick="selectModel('deepseek3')">
                <div class="model-title">🧠 DeepSeek-V3-Base</div>
                <div><strong>China's MoE Champion</strong></div>
                <div>Multi-Head Latent Attention</div>
                <div class="model-specs">
                    61 layers • 7168 d_model<br>
                    128 heads • MLA compression<br>
                    RMSNorm • SwiGLU • RoPE<br>
                    128K context • 671B total / 37B active
                </div>
            </div>
            
            <div class="model-card" onclick="selectModel('mistral3')">
                <div class="model-title">🌪️ Mistral-Small-3.1</div>
                <div><strong>French Efficiency</strong></div>
                <div>Speed-optimized architecture</div>
                <div class="model-specs">
                    32 layers • 4096 d_model<br>
                    32 heads • 8 KV groups (GQA)<br>
                    RMSNorm • SwiGLU • RoPE<br>
                    128K context • 22B params
                </div>
            </div>
            
            <div class="model-card" onclick="selectModel('llama4')">
                <div class="model-title">🦙 Llama 4 Maverick</div>
                <div><strong>Meta's MoE Entry</strong></div>
                <div>Latest 2025 architecture</div>
                <div class="model-specs">
                    126 layers • 8192 d_model<br>
                    128 heads • 16 KV groups (GQA)<br>
                    RMSNorm • SwiGLU • RoPE<br>
                    128K context • 400B total / 17B active
                </div>
            </div>
        </div>
        
        <div id="selectedModels"></div>
        
        <button onclick="compareSelected()">🔍 Compare Selected Models</button>
        <div id="comparisonResults"></div>
    </div>

    <div class="container">
        <h2>📈 The Evolution: What Actually Changed (2020-2025)?</h2>
        
        <div class="evolution-timeline">
            <div class="timeline-item" onclick="showInnovation('gpt3')">
                <div class="timeline-year">2020</div>
                <div class="timeline-model">GPT-3</div>
                <div class="timeline-innovation">Scale breakthrough</div>
            </div>
            <div class="timeline-item" onclick="showInnovation('palm')">
                <div class="timeline-year">2022</div>
                <div class="timeline-model">PaLM</div>
                <div class="timeline-innovation">Pure scale + quality</div>
            </div>
            <div class="timeline-item" onclick="showInnovation('llama1')">
                <div class="timeline-year">2023</div>
                <div class="timeline-model">LLaMA-1</div>
                <div class="timeline-innovation">Efficient training</div>
            </div>
            <div class="timeline-item" onclick="showInnovation('llama2')">
                <div class="timeline-year">2023</div>
                <div class="timeline-model">LLaMA-2</div>
                <div class="timeline-innovation">GQA introduction</div>
            </div>
            <div class="timeline-item" onclick="showInnovation('gemma2')">
                <div class="timeline-year">2024</div>
                <div class="timeline-model">Gemma 2</div>
                <div class="timeline-innovation">Sliding window</div>
            </div>
            <div class="timeline-item" onclick="showInnovation('deepseek3')">
                <div class="timeline-year">2024</div>
                <div class="timeline-model">DeepSeek-V3</div>
                <div class="timeline-innovation">MLA + MoE</div>
            </div>
        </div>
        
        <div id="innovationDetails"></div>
        
        <div class="success">
            <strong>🎯 Key Insight:</strong> Most changes are incremental optimizations (GQA, sliding window, different layer norms) rather than fundamental architectural shifts. The original transformer paradigm remains remarkably durable!
        </div>
    </div>

    <div class="container">
        <h2>🔍 Component-by-Component Analysis</h2>
        
        <div class="step">
            <h3>⚡ Attention Mechanisms</h3>
            
            <div class="controls">
                <div class="control-group">
                    <label><strong>Focus Component:</strong></label>
                    <select id="componentFocus">
                        <option value="attention" selected>Attention Mechanisms</option>
                        <option value="activation">Activation Functions</option>
                        <option value="normalization">Normalization Layers</option>
                        <option value="position">Position Encodings</option>
                        <option value="architecture">Overall Architecture</option>
                    </select>
                </div>
            </div>
            
            <button onclick="analyzeComponent()">🔬 Analyze Component Differences</button>
            <div id="componentAnalysis"></div>
        </div>
    </div>

    <div class="container">
        <h2>📊 Performance vs Architecture Trade-offs</h2>
        
        <div class="step">
            <h3>🎯 Real-World Implications</h3>
            
            <div class="controls">
                <div class="control-group">
                    <label><strong>Analysis Type:</strong></label>
                    <select id="analysisType">
                        <option value="memory">Memory Efficiency</option>
                        <option value="speed" selected>Inference Speed</option>
                        <option value="quality">Model Quality</option>
                        <option value="training">Training Efficiency</option>
                    </select>
                </div>
                <div class="control-group">
                    <label><strong>Context Length:</strong></label>
                    <select id="contextLen">
                        <option value="2048">2K tokens</option>
                        <option value="8192" selected>8K tokens</option>
                        <option value="32768">32K tokens</option>
                        <option value="131072">128K tokens</option>
                    </select>
                </div>
            </div>
            
            <button onclick="analyzeTradeoffs()">📈 Analyze Performance Trade-offs</button>
            <div id="tradeoffAnalysis"></div>
        </div>
    </div>

    <div class="container">
        <h2>🎮 Interactive Architecture Explorer</h2>
        
        <div class="step">
            <h3>🔍 Deep Dive Comparison</h3>
            
            <div class="warning">
                <strong>🎯 Pro Tip:</strong> Select two models above, then use this explorer to understand exactly how their architectures differ and why those differences matter.
            </div>
            
            <button onclick="launchExplorer()">🚀 Launch Side-by-Side Architecture Explorer</button>
            <div id="architectureExplorer"></div>
        </div>
    </div>

    <script>
        // Model configurations with real specifications
        const modelConfigs = {
            'llama3': {
                name: 'LLaMA-3-8B',
                organization: 'Meta',
                year: 2024,
                layers: 32,
                d_model: 4096,
                n_heads: 32,
                n_kv_heads: 8,
                vocab_size: 128256,
                d_ff: 14336,
                max_context: 131072,
                total_params: 8.03,
                attention_type: 'GQA',
                activation: 'SwiGLU',
                norm: 'RMSNorm',
                position: 'RoPE',
                innovations: ['GQA', 'Large vocabulary', 'Long context'],
                architecture_type: 'Dense Decoder-Only'
            },
            'qwen3': {
                name: 'Qwen3-8B',
                organization: 'Alibaba',
                year: 2024,
                layers: 28,
                d_model: 4096,
                n_heads: 32,
                n_kv_heads: 8,
                vocab_size: 151936,
                d_ff: 22016,
                max_context: 131072,
                total_params: 8.1,
                attention_type: 'GQA',
                activation: 'SwiGLU',
                norm: 'RMSNorm',
                position: 'RoPE',
                innovations: ['Massive vocabulary', 'Efficient FFN', 'Fewer layers'],
                architecture_type: 'Dense Decoder-Only'
            },
            'gemma3': {
                name: 'Gemma 3-9B',
                organization: 'Google',
                year: 2024,
                layers: 42,
                d_model: 3584,
                n_heads: 16,
                n_kv_heads: 16,
                vocab_size: 256000,
                d_ff: 14336,
                max_context: 8192,
                total_params: 9.0,
                attention_type: 'Sliding Window GQA',
                activation: 'GeGLU',
                norm: 'RMSNorm (Pre+Post)',
                position: 'RoPE',
                innovations: ['Sliding window attention', 'Pre+Post norm', 'GeGLU'],
                architecture_type: 'Dense Decoder-Only',
                sliding_window: 1024
            },
            'deepseek3': {
                name: 'DeepSeek-V3',
                organization: 'DeepSeek',
                year: 2024,
                layers: 61,
                d_model: 7168,
                n_heads: 128,
                n_kv_heads: 'MLA',
                vocab_size: 129280,
                d_ff: 18432,
                max_context: 131072,
                total_params: 671,
                active_params: 37,
                attention_type: 'Multi-Head Latent Attention (MLA)',
                activation: 'SwiGLU',
                norm: 'RMSNorm',
                position: 'RoPE',
                innovations: ['MLA compression', 'MoE architecture', 'Shared expert'],
                architecture_type: 'MoE Decoder-Only',
                experts: 256,
                active_experts: 8,
                mla_compress_dim: 1792
            },
            'mistral3': {
                name: 'Mistral-Small-3.1',
                organization: 'Mistral',
                year: 2024,
                layers: 32,
                d_model: 4096,
                n_heads: 32,
                n_kv_heads: 8,
                vocab_size: 32768,
                d_ff: 14336,
                max_context: 131072,
                total_params: 22,
                attention_type: 'GQA',
                activation: 'SwiGLU',
                norm: 'RMSNorm',
                position: 'RoPE',
                innovations: ['Speed optimization', 'Custom tokenizer', 'Reduced KV cache'],
                architecture_type: 'Dense Decoder-Only'
            },
            'llama4': {
                name: 'Llama 4 Maverick',
                organization: 'Meta',
                year: 2025,
                layers: 126,
                d_model: 8192,
                n_heads: 128,
                n_kv_heads: 16,
                vocab_size: 128256,
                d_ff: 65536,
                max_context: 131072,
                total_params: 400,
                active_params: 17,
                attention_type: 'GQA',
                activation: 'SwiGLU',
                norm: 'RMSNorm',
                position: 'RoPE',
                innovations: ['Alternating MoE/Dense', 'Massive scale', 'Efficient routing'],
                architecture_type: 'Hybrid MoE Decoder-Only',
                experts: 16,
                active_experts: 2
            }
        };

        let selectedModels = [];

        function selectModel(modelId) {
            const card = event.target.closest('.model-card');
            
            if (selectedModels.includes(modelId)) {
                // Deselect
                selectedModels = selectedModels.filter(id => id !== modelId);
                card.classList.remove('selected');
            } else if (selectedModels.length < 3) {
                // Select (max 3)
                selectedModels.push(modelId);
                card.classList.add('selected');
            } else {
                alert('Maximum 3 models for comparison. Deselect one first.');
                return;
            }
            
            updateSelectedDisplay();
        }

        function updateSelectedDisplay() {
            let html = '';
            if (selectedModels.length > 0) {
                html = '<div class="info"><strong>Selected for Comparison:</strong> ';
                html += selectedModels.map(id => modelConfigs[id].name).join(', ');
                html += ' (' + selectedModels.length + '/3)</div>';
            }
            document.getElementById('selectedModels').innerHTML = html;
        }

        function compareSelected() {
            if (selectedModels.length < 2) {
                alert('Please select at least 2 models to compare.');
                return;
            }
            
            let html = '<div class="step"><h3>🔍 Side-by-Side Comparison</h3>';
            
            // Create comparison table
            html += '<table>';
            html += '<tr><th>Specification</th>';
            selectedModels.forEach(id => {
                html += '<th>' + modelConfigs[id].name + '</th>';
            });
            html += '</tr>';
            
            // Key specifications
            const specs = [
                { key: 'organization', label: 'Organization' },
                { key: 'year', label: 'Release Year' },
                { key: 'layers', label: 'Layers' },
                { key: 'd_model', label: 'd_model' },
                { key: 'n_heads', label: 'Attention Heads' },
                { key: 'n_kv_heads', label: 'KV Heads' },
                { key: 'attention_type', label: 'Attention Type' },
                { key: 'activation', label: 'Activation' },
                { key: 'norm', label: 'Normalization' },
                { key: 'vocab_size', label: 'Vocabulary Size' },
                { key: 'max_context', label: 'Max Context' },
                { key: 'total_params', label: 'Parameters (B)' },
                { key: 'architecture_type', label: 'Architecture' }
            ];
            
            specs.forEach(spec => {
                html += '<tr><td><strong>' + spec.label + '</strong></td>';
                
                const values = selectedModels.map(id => {
                    const model = modelConfigs[id];
                    let value = model[spec.key];
                    
                    if (spec.key === 'vocab_size' || spec.key === 'max_context') {
                        value = typeof value === 'number' ? value.toLocaleString() : value;
                    }
                    
                    return value;
                });
                
                // Highlight differences
                const allSame = values.every(v => v === values[0]);
                
                values.forEach(value => {
                    const cellClass = allSame ? '' : 'model-highlight';
                    html += '<td class="' + cellClass + '">' + value + '</td>';
                });
                
                html += '</tr>';
            });
            
            html += '</table>';
            
            // Key differences analysis
            html += '<h4>🎯 Key Architectural Differences</h4>';
            
            const differences = analyzeKeyDifferences(selectedModels);
            html += '<div class="component-comparison">';
            differences.forEach(diff => {
                html += '<div class="innovation-highlight">';
                html += '<span class="difference-indicator ' + diff.type + '"></span>';
                html += '<strong>' + diff.title + ':</strong> ' + diff.description;
                html += '</div>';
            });
            html += '</div>';
            
            // Performance implications
            html += '<h4>⚡ Performance Implications</h4>';
            html += '<div class="parameter-breakdown">';
            
            selectedModels.forEach(id => {
                const model = modelConfigs[id];
                const isMemoryEfficient = ['deepseek3', 'gemma3'].includes(id);
                const isSpeedOptimized = ['mistral3', 'qwen3'].includes(id);
                const isScaled = ['llama4', 'deepseek3'].includes(id);
                
                html += '<div class="param-card">';
                html += '<div class="param-title">' + model.name + '</div>';
                html += '<div class="param-value">' + model.total_params + 'B</div>';
                html += '<div class="param-desc">';
                if (isMemoryEfficient) html += '💾 Memory Efficient<br>';
                if (isSpeedOptimized) html += '⚡ Speed Optimized<br>';
                if (isScaled) html += '📈 Massive Scale<br>';
                html += model.architecture_type;
                html += '</div>';
                html += '</div>';
            });
            
            html += '</div>';
            html += '</div>';
            
            document.getElementById('comparisonResults').innerHTML = html;
        }

        function analyzeKeyDifferences(modelIds) {
            const models = modelIds.map(id => modelConfigs[id]);
            const differences = [];
            
            // Attention mechanism differences
            const attentionTypes = [...new Set(models.map(m => m.attention_type))];
            if (attentionTypes.length > 1) {
                differences.push({
                    type: 'different',
                    title: 'Attention Mechanisms',
                    description: `Different approaches: ${attentionTypes.join(' vs ')}. MLA (DeepSeek) compresses KV cache, GQA reduces memory, Sliding Window (Gemma) limits context per layer.`
                });
            }
            
            // Architecture type differences
            const archTypes = [...new Set(models.map(m => m.architecture_type))];
            if (archTypes.length > 1) {
                differences.push({
                    type: 'different',
                    title: 'Architecture Types',
                    description: `Mix of ${archTypes.join(' and ')}. MoE models use sparse computation (only some parameters active), Dense models use all parameters.`
                });
            }
            
            // Scale differences
            const paramRanges = models.map(m => m.total_params);
            const minParams = Math.min(...paramRanges);
            const maxParams = Math.max(...paramRanges);
            if (maxParams / minParams > 2) {
                differences.push({
                    type: 'different',
                    title: 'Scale Variation',
                    description: `Parameter counts range from ${minParams}B to ${maxParams}B (${(maxParams/minParams).toFixed(1)}x difference). Larger models generally more capable but slower.`
                });
            }
            
            // Vocabulary size differences
            const vocabSizes = models.map(m => m.vocab_size);
            const minVocab = Math.min(...vocabSizes);
            const maxVocab = Math.max(...vocabSizes);
            if (maxVocab / minVocab > 1.5) {
                differences.push({
                    type: 'similar',
                    title: 'Vocabulary Sizes',
                    description: `Range from ${minVocab.toLocaleString()} to ${maxVocab.toLocaleString()} tokens. Larger vocabularies enable better multilingual support and compression.`
                });
            }
            
            // Layer count differences
            const layerCounts = models.map(m => m.layers);
            const minLayers = Math.min(...layerCounts);
            const maxLayers = Math.max(...layerCounts);
            if (maxLayers / minLayers > 1.5) {
                differences.push({
                    type: 'similar',
                    title: 'Layer Counts',
                    description: `From ${minLayers} to ${maxLayers} layers. More layers = deeper reasoning but slower inference. Qwen uses fewer, deeper layers efficiently.`
                });
            }
            
            return differences;
        }

        function showInnovation(model) {
            let html = '<div class="step"><h3>💡 ' + model.toUpperCase() + ' Innovation Deep Dive</h3>';
            
            switch(model) {
                case 'gpt3':
                    html += '<div class="example-box">';
                    html += '<strong>GPT-3 (2020): The Scale Breakthrough</strong><br>';
                    html += '• 175B parameters (100x larger than GPT-2)<br>';
                    html += '• Proved scaling laws work<br>';
                    html += '• Few-shot learning without fine-tuning<br>';
                    html += '• Standard transformer architecture, just bigger<br>';
                    html += '• Established the "scale = capability" paradigm';
                    html += '</div>';
                    break;
                    
                case 'llama1':
                    html += '<div class="example-box">';
                    html += '<strong>LLaMA-1 (2023): Efficient Training Revolution</strong><br>';
                    html += '• Better performance with fewer parameters than GPT-3<br>';
                    html += '• RMSNorm instead of LayerNorm<br>';
                    html += '• SwiGLU activation function<br>';
                    html += '• RoPE position embeddings<br>';
                    html += '• Proved efficiency > pure scale';
                    html += '</div>';
                    break;
                    
                case 'llama2':
                    html += '<div class="example-box">';
                    html += '<strong>LLaMA-2 (2023): GQA Introduction</strong><br>';
                    html += '• Grouped Query Attention for memory efficiency<br>';
                    html += '• 4:1 ratio (32 query heads, 8 KV heads)<br>';
                    html += '• 70B model with manageable KV cache<br>';
                    html += '• Conversation fine-tuning (Chat models)<br>';
                    html += '• Open-source competitive with GPT-3.5';
                    html += '</div>';
                    break;
                    
                case 'gemma2':
                    html += '<div class="example-box">';
                    html += '<strong>Gemma 2 (2024): Sliding Window Innovation</strong><br>';
                    html += '• Sliding window attention (4K local window)<br>';
                    html += '• Alternating global/local attention layers<br>';
                    html += '• Pre + Post normalization<br>';
                    html += '• Excellent performance at 27B scale<br>';
                    html += '• Memory efficiency without quality loss';
                    html += '</div>';
                    break;
                    
                case 'deepseek3':
                    html += '<div class="example-box">';
                    html += '<strong>DeepSeek-V3 (2024): MLA + MoE Mastery</strong><br>';
                    html += '• Multi-Head Latent Attention (93% KV cache reduction)<br>';
                    html += '• 256 experts with 8+1 active (shared expert)<br>';
                    html += '• 671B total parameters, 37B active<br>';
                    html += '• Outperforms much larger dense models<br>';
                    html += '• New paradigm: compression + sparsity';
                    html += '</div>';
                    break;
                    
                default:
                    html += '<div class="example-box">Select a model from the timeline to see its innovations!</div>';
            }
            
            html += '</div>';
            document.getElementById('innovationDetails').innerHTML = html;
        }

        function analyzeComponent() {
            const component = document.getElementById('componentFocus').value;
            
            let html = '<div class="step"><h3>🔬 ' + component.toUpperCase() + ' Analysis</h3>';
            
            switch(component) {
                case 'attention':
                    html += '<table>';
                    html += '<tr><th>Model</th><th>Attention Type</th><th>Memory Strategy</th><th>KV Cache Size</th></tr>';
                    
                    Object.entries(modelConfigs).forEach(([id, model]) => {
                        const kvCacheReduction = model.attention_type.includes('MLA') ? '93% smaller' : 
                                               model.attention_type.includes('GQA') ? '4x smaller' :
                                               model.attention_type.includes('Sliding') ? '75% smaller' : 'Standard';
                        
                        html += '<tr>';
                        html += '<td><strong>' + model.name + '</strong></td>';
                        html += '<td>' + model.attention_type + '</td>';
                        html += '<td>' + (model.attention_type.includes('MLA') ? 'Compression' : 
                                         model.attention_type.includes('GQA') ? 'Head Grouping' :
                                         model.attention_type.includes('Sliding') ? 'Local Windows' : 'Full Attention') + '</td>';
                        html += '<td>' + kvCacheReduction + '</td>';
                        html += '</tr>';
                    });
                    
                    html += '</table>';
                    
                    html += '<div class="success">';
                    html += '<strong>🎯 Attention Evolution:</strong> The trend is clear - reduce KV cache memory while maintaining quality. MLA (compression) > GQA (grouping) > Sliding Window (locality) > Standard MHA.';
                    html += '</div>';
                    break;
                    
                case 'activation':
                    html += '<div class="component-comparison">';
                    html += '<div class="component-title">Activation Function Evolution</div>';
                    
                    const activations = {};
                    Object.values(modelConfigs).forEach(model => {
                        if (!activations[model.activation]) {
                            activations[model.activation] = [];
                        }
                        activations[model.activation].push(model.name);
                    });
                    
                    Object.entries(activations).forEach(([activation, models]) => {
                        html += '<div class="innovation-highlight">';
                        html += '<strong>' + activation + ':</strong> ' + models.join(', ');
                        if (activation === 'SwiGLU') {
                            html += ' (Most popular - better than ReLU/GELU)';
                        } else if (activation === 'GeGLU') {
                            html += ' (Google variant - similar performance)';
                        }
                        html += '</div>';
                    });
                    
                    html += '</div>';
                    break;
                    
                case 'normalization':
                    html += '<div class="component-comparison">';
                    html += '<div class="component-title">Normalization Strategies</div>';
                    
                    Object.values(modelConfigs).forEach(model => {
                        html += '<div class="innovation-highlight">';
                        html += '<strong>' + model.name + ':</strong> ' + model.norm;
                        if (model.norm.includes('Pre+Post')) {
                            html += ' (Hybrid approach for stability)';
                        } else {
                            html += ' (Standard Pre-LN)';
                        }
                        html += '</div>';
                    });
                    
                    html += '</div>';
                    
                    html += '<div class="info">';
                    html += '<strong>🔄 Consensus:</strong> Almost everyone uses RMSNorm (simpler than LayerNorm). Gemma experiments with Pre+Post placement for extra stability.';
                    html += '</div>';
                    break;
                    
                case 'position':
                    html += '<div class="success">';
                    html += '<strong>🎯 Universal Adoption:</strong> RoPE (Rotary Position Embedding) has won. Every modern model uses RoPE because it enables length extrapolation and relative position understanding.';
                    html += '</div>';
                    break;
                    
                case 'architecture':
                    html += '<div class="parameter-breakdown">';
                    
                    const denseModels = Object.values(modelConfigs).filter(m => m.architecture_type.includes('Dense'));
                    const moeModels = Object.values(modelConfigs).filter(m => m.architecture_type.includes('MoE'));
                    
                    html += '<div class="param-card">';
                    html += '<div class="param-title">Dense Models</div>';
                    html += '<div class="param-value">' + denseModels.length + '</div>';
                    html += '<div class="param-desc">Simple, reliable, easy to optimize</div>';
                    html += '</div>';
                    
                    html += '<div class="param-card">';
                    html += '<div class="param-title">MoE Models</div>';
                    html += '<div class="param-value">' + moeModels.length + '</div>';
                    html += '<div class="param-desc">Massive scale, sparse computation</div>';
                    html += '</div>';
                    
                    html += '</div>';
                    
                    html += '<div class="warning">';
                    html += '<strong>🎯 The Big Divide:</strong> Dense models (LLaMA, Qwen, Gemma) vs MoE models (DeepSeek, Llama 4). MoE enables massive parameter counts but adds complexity.';
                    html += '</div>';
                    break;
            }
            
            html += '</div>';
            document.getElementById('componentAnalysis').innerHTML = html;
        }

        function analyzeTradeoffs() {
            const analysisType = document.getElementById('analysisType').value;
            const contextLen = parseInt(document.getElementById('contextLen').value);
            
            let html = '<div class="step"><h3>📈 ' + analysisType.toUpperCase() + ' Analysis</h3>';
            
            if (analysisType === 'memory') {
                html += '<table>';
                html += '<tr><th>Model</th><th>KV Cache (MB)</th><th>Model Size (GB)</th><th>Total Memory (GB)</th><th>Efficiency</th></tr>';
                
                Object.values(modelConfigs).forEach(model => {
                    // KV cache calculation (simplified)
                    const kvCacheSize = calculateKVCache(model, contextLen);
                    const modelSize = model.total_params * 2; // FP16
                    const totalMemory = modelSize + kvCacheSize / 1024;
                    
                    const efficiency = model.attention_type.includes('MLA') ? '⭐⭐⭐' :
                                     model.attention_type.includes('GQA') ? '⭐⭐' :
                                     model.attention_type.includes('Sliding') ? '⭐⭐' : '⭐';
                    
                    html += '<tr>';
                    html += '<td><strong>' + model.name + '</strong></td>';
                    html += '<td>' + kvCacheSize.toFixed(0) + '</td>';
                    html += '<td>' + modelSize.toFixed(1) + '</td>';
                    html += '<td>' + totalMemory.toFixed(1) + '</td>';
                    html += '<td>' + efficiency + '</td>';
                    html += '</tr>';
                });
                
                html += '</table>';
                
            } else if (analysisType === 'speed') {
                html += '<div class="parameter-breakdown">';
                
                Object.values(modelConfigs).forEach(model => {
                    const speedScore = calculateSpeedScore(model);
                    const speedRating = speedScore > 8 ? 'Very Fast' : speedScore > 6 ? 'Fast' : speedScore > 4 ? 'Medium' : 'Slow';
                    
                    html += '<div class="param-card">';
                    html += '<div class="param-title">' + model.name + '</div>';
                    html += '<div class="param-value">' + speedRating + '</div>';
                    html += '<div class="param-desc">';
                    if (model.total_params < 10) html += 'Small model<br>';
                    if (model.attention_type.includes('GQA')) html += 'GQA efficient<br>';
                    if (model.name.includes('Mistral')) html += 'Speed optimized<br>';
                    html += speedScore.toFixed(1) + '/10 score';
                    html += '</div>';
                    html += '</div>';
                });
                
                html += '</div>';
                
            } else if (analysisType === 'quality') {
                html += '<div class="info">';
                html += '<strong>📊 Quality Hierarchy (2024-2025):</strong> Based on benchmark averages and real-world performance.';
                html += '</div>';
                
                const qualityRanking = [
                    { model: 'DeepSeek-V3', score: 95, note: 'Massive MoE with MLA - top tier' },
                    { model: 'Llama 4 Maverick', score: 92, note: 'Latest Meta MoE - excellent' },
                    { model: 'Qwen3-8B', score: 88, note: 'Best dense 8B model' },
                    { model: 'LLaMA-3-8B', score: 85, note: 'Solid baseline, well-tested' },
                    { model: 'Gemma 3-9B', score: 83, note: 'Good but sliding window limits' },
                    { model: 'Mistral-Small-3.1', score: 80, note: 'Speed over absolute quality' }
                ];
                
                html += '<table>';
                html += '<tr><th>Rank</th><th>Model</th><th>Quality Score</th><th>Notes</th></tr>';
                
                qualityRanking.forEach((item, idx) => {
                    html += '<tr>';
                    html += '<td><strong>' + (idx + 1) + '</strong></td>';
                    html += '<td>' + item.model + '</td>';
                    html += '<td>' + item.score + '/100</td>';
                    html += '<td>' + item.note + '</td>';
                    html += '</tr>';
                });
                
                html += '</table>';
                
            } else {
                html += '<div class="component-comparison">';
                html += '<div class="component-title">Training Efficiency Factors</div>';
                
                html += '<div class="innovation-highlight">';
                html += '<strong>Most Efficient:</strong> Qwen3 (fewer layers, larger FFN), Mistral (optimized architecture)';
                html += '</div>';
                
                html += '<div class="innovation-highlight">';
                html += '<strong>Most Complex:</strong> DeepSeek-V3 (MoE + MLA), Llama 4 (massive MoE)';
                html += '</div>';
                
                html += '<div class="innovation-highlight">';
                html += '<strong>Best Balance:</strong> LLaMA-3 (proven architecture), Gemma 3 (memory efficient)';
                html += '</div>';
                
                html += '</div>';
            }
            
            html += '</div>';
            document.getElementById('tradeoffAnalysis').innerHTML = html;
        }

        function calculateKVCache(model, contextLen) {
            // Simplified KV cache calculation in MB
            if (model.attention_type.includes('MLA')) {
                // MLA: compressed cache
                return contextLen * model.mla_compress_dim * 2 * 2 / 1024 / 1024;
            } else if (model.attention_type.includes('GQA')) {
                // GQA: reduced KV heads
                return contextLen * model.n_kv_heads * (model.d_model / model.n_heads) * 2 * 2 / 1024;
            } else {
                // Standard attention
                return contextLen * model.n_heads * (model.d_model / model.n_heads) * 2 * 2 / 1024;
            }
        }

        function calculateSpeedScore(model) {
            let score = 10;
            
            // Penalize for size
            if (model.total_params > 50) score -= 4;
            else if (model.total_params > 20) score -= 2;
            else if (model.total_params > 10) score -= 1;
            
            // Bonus for efficiency features
            if (model.attention_type.includes('GQA')) score += 1;
            if (model.attention_type.includes('MLA')) score += 0.5;
            if (model.name.includes('Mistral')) score += 1; // Speed optimized
            
            // Penalize for complexity
            if (model.architecture_type.includes('MoE')) score -= 1;
            if (model.layers > 50) score -= 0.5;
            
            return Math.max(0, Math.min(10, score));
        }

        function launchExplorer() {
            if (selectedModels.length < 2) {
                alert('Please select at least 2 models first to use the architecture explorer.');
                return;
            }
            
            let html = '<div class="step"><h3>🚀 Side-by-Side Architecture Explorer</h3>';
            
            html += '<div class="side-by-side">';
            
            selectedModels.slice(0, 2).forEach(modelId => {
                const model = modelConfigs[modelId];
                
                html += '<div class="architecture-diagram">';
                html += '<h4>' + model.name + '</h4>';
                
                // Architecture flow
                html += '<div class="layer-box">🎯 Output Logits</div>';
                html += '<div class="arrow-down">↓</div>';
                html += '<div class="layer-box">📊 Language Model Head</div>';
                html += '<div class="arrow-down">↓</div>';
                
                if (model.architecture_type.includes('MoE')) {
                    for (let i = 0; i < 3; i++) {
                        html += '<div class="layer-box layer-highlight">🎯 MoE Layer ' + (model.layers - i) + '</div>';
                        html += '<div class="arrow-down">↓</div>';
                    }
                    html += '<div class="layer-box">⋮ ' + (model.layers - 6) + ' more layers</div>';
                    html += '<div class="arrow-down">↓</div>';
                } else {
                    html += '<div class="layer-box">🏗️ Transformer Layer ' + model.layers + '</div>';
                    html += '<div class="arrow-down">↓</div>';
                    html += '<div class="layer-box">⋮ ' + (model.layers - 1) + ' more layers</div>';
                    html += '<div class="arrow-down">↓</div>';
                }
                
                html += '<div class="layer-box">🏗️ Transformer Layer 1</div>';
                html += '<div class="arrow-down">↓</div>';
                
                if (model.attention_type.includes('MLA')) {
                    html += '<div class="layer-box layer-difference">🧠 MLA Attention</div>';
                } else if (model.attention_type.includes('Sliding')) {
                    html += '<div class="layer-box layer-difference">🪟 Sliding Window Attention</div>';
                } else {
                    html += '<div class="layer-box">👁️ ' + model.attention_type + '</div>';
                }
                
                html += '<div class="arrow-down">↓</div>';
                html += '<div class="layer-box">⚡ ' + model.activation + '</div>';
                html += '<div class="arrow-down">↓</div>';
                html += '<div class="layer-box">📍 RoPE Position</div>';
                html += '<div class="arrow-down">↓</div>';
                html += '<div class="layer-box">📚 Token Embeddings</div>';
                html += '<div class="arrow-down">↓</div>';
                html += '<div class="layer-box">📝 Input Tokens</div>';
                
                html += '</div>';
            });
            
            html += '</div>';
            
            // Key differences summary
            html += '<div class="success">';
            html += '<strong>🎯 Key Architectural Differences:</strong><br>';
            const model1 = modelConfigs[selectedModels[0]];
            const model2 = modelConfigs[selectedModels[1]];
            
            if (model1.attention_type !== model2.attention_type) {
                html += '• <strong>Attention:</strong> ' + model1.attention_type + ' vs ' + model2.attention_type + '<br>';
            }
            if (model1.architecture_type !== model2.architecture_type) {
                html += '• <strong>Architecture:</strong> ' + model1.architecture_type + ' vs ' + model2.architecture_type + '<br>';
            }
            if (Math.abs(model1.total_params - model2.total_params) > 10) {
                html += '• <strong>Scale:</strong> ' + model1.total_params + 'B vs ' + model2.total_params + 'B parameters<br>';
            }
            html += '</div>';
            
            html += '</div>';
            document.getElementById('architectureExplorer').innerHTML = html;
        }

        // Initialize with default innovation
        document.addEventListener('DOMContentLoaded', function() {
            showInnovation('deepseek3');
        });
    </script>
</body>
</html>
